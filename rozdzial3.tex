\chapter{Metoda estymatora ważonego}
\thispagestyle{chapterBeginStyle}




W tym rozdziale zajmiemy się metodą estymacji operacji teoriomnogościowych opisaną w \cite{ting}. Estymatory oparte na tej metodzie są definiowane jako średnia ważonej pewnej liczby  tzw. estymatorów składowych. Opiszemy w jaki sposób można zdefiniować estymatory składowe oraz jak jest optymalny sposób ich ważenia. 

\section{Optymalne ważenie estymatorów}
Na początku wyjaśnimy na czym polega ważenie estymatorów.
W tym celu przypomnijmy definicje \textit{macierz kowariancji}. Macierz ta jest uogólnieniem pojęcia wariancji na przypadek wielowymiarowy. Dla wektora zmiennych losowych $X_1, X_2, \ldots, X_n$ macierz kowariancji ma postać:\JL{poprawić tą macierz}
\[
\Sigma =
  \begin{bmatrix}
    {{\sigma}_1}^2 & {\sigma}_{12} & \cdots & {\sigma}_{1n}  \\
    {{\sigma}_21} & {{\sigma}_{2}}^2 & \cdots & {\sigma}_{2n} \\
    \vdots & \cdots & \ddots & \vdots \\
    {{\sigma}_n1} & {{\sigma}_{n2}} & \cdots & {{\sigma}_{n}^2}
  \end{bmatrix},
\]
gdzie
    ${{\sigma}_i}^2 = Var(X_i)$,
    ${\sigma}_{ij} = Cov(X_i, X_j)$.

Warto dodać, że macierz kowariancji $\Sigma$ jest macierzą symetryczną, a jej wyznacznik jest nieujemny (jeżeli wektor zmiennych losowych jest niezdegenerowany). \JL{czy powyższa uwaga jest do czegokolwiek przydatna, a jesli tak to co to znaczy wektor niezdegenerowany?} 

Poniżej prezentujemy lemat z pracy  \cite{ting}

\begin{lemma}
content...
\end{lemma}

Zgodne z lematem 3 \cite{ting}, posiadając ciąg zgodnych estymatorów $\hat{N_1}, ..., \hat{N_n}$ o nieosobliwej macierzy kowariancji $\Sigma$, optymalne wagi które sumują się do $1$ i minimalizują wariancję są proporcjonalne do $\Sigma^{-1}1_n$, gdzie $1_n$ to wektor jednostkowy długości $n$. Powstały estymator jest nieobciążony (bądź zgodny) o wariancji ${{1_n}^{T}\Sigma^{-1}1_n}^{-1}$.
\JL{Zamiast tego całego akapitu napisać Lemat i jego tresc. I żeby miało to sens, bo w tej chwili to co jest napisane nie ma sensu...}


W praktyce jednak macierz kowariancji $\Sigma$ rzadko jest znana i potrzebna jest jakaś jej aproksymacja.
Jeśli przyjemy, że estymatory składowe są niezależne
lub prawie niezależne, możemy poprzestać na wyliczeniu przekątnej macierzy kowariancji, czyli wariancji estymatorów składowych. Wówczas, \textit{estymator ważony} $\hat{N}_w$  ma następującą postać:
\begin{equation}
\label{wazony}
    \hat{N}_w = \sum_{i=1}^{n}c\frac{\hat{N_i}(S)}{Var(\hat{N_i}(S))},
\end{equation}
\JL{tzreba by jeszcze zdefinowac co znaczy $\hat{N_i}(S)$ ), bo to chyba nowe oznaczenie jest, czym jest S wyjasnic } 
gdzie $c$ to stała normalizacyjna, zapewniająca, że wagi sumują się do jedynki.

\section{Estymatory składowe dla algorytmu \texttt{Streaming MinCount}}

W dalszej części tego rozdziału, dotyczącej szkiców algorytmu \texttt{MinCount}, zakładamy użycie algorytmu \texttt{Streaming MinCount} opisanego w rozdziale $2.3$.
Estymator w tej wersji algorytmu jest bardziej skoncentrowany, a ponadto, co okaże się istotne, jego rozkład może być aproksymowany przez rozkład normalny\JL{odniesienie do wykresu}. 

Omówimy teraz metodę  definiowania estymatorów składowych. Naszym celem jest zdefiniowanie estymatorów opartach na dostępnym szkicu, które są jak najmniej nawzajem ze sobą skorelowane, oraz takich, których wariancję jesteśmy w stanie efektywnie wyliczyć lub przybliżyć. Poniżej przedstawiamy pomysł z pracy \cite{ting} umożliwiający wyrażenie estymatora ważonego sumy i przekroju jako  liniowej kombinacją  estymatorów liczności opartych na szkicach \texttt{MinCount}.

Aby zdefiniować estymatory składowe dla sumy i dla przekroju  zauważmy, że zarówno liczność sumy dwóch zbiorów, jak i ich przekroju możemy zapisać jako stosunek liczności sumy (analogicznie - przekroju) do liczności jednego z tych zbiorów pomnożonej przez liczność tegoż zbioru. Dla
\begin{flalign}
       {\alpha}_{i} = \frac{|A_1 \cap A_2|}{|A_i|}, \\
    {\beta}_{i} = \frac{|A_1 \cup A_2|}{|A_i|}
\end{flalign}
mamy zatem
\begin{flalign}
       |A_1 \cap A_2| = {\alpha}_{i}|A_i| = \frac{|A_1 \cap A_2|}{|A_i|}|A_i|, \\
    |A_1 \cup A_2| = {\beta}_{i}|A_i| = \frac{|A_1 \cup A_2|}{|A_i|}|A_i|.
\end{flalign}
 Naturalnymi estymatorami dla $\alpha_i$ oraz $\beta_i$ zdefiniowanymi w oparciu o szkice \texttt{MinCount} wydają się być:
\begin{flalign}
     \hat{{\alpha}_{i}} = \frac{|h(S_1 \cap S_2)|}{|h(S_i, {\tau}_{min})|},\\
    \hat{{\beta}_{i}} = \frac{|h(S_1 \cup S_2)|}{|h(S_i, {\tau}_{min})|}.
\end{flalign}
\JL{Wyjasnić skad sie wzieły te wzory!!!}
\JL{Poprawic oznaczenia we wzorach, operacja sumy i przekroju na szkicach}
\JL{zrobic odwolanie do wzoru \ref{sketch-cut} }
Powyższe estymatory są estymatorami zgodnymi \cite{ting}. 

Mając estymatory dla $\alpha_i$ i $\beta_i$ oraz estymatory liczności $\hat{N}(S_i)$ zbioru $A_i$ możemy zdefiniować estymatory składowe dla sumy i przekroju w następującej postaci:
\begin{flalign}
     \hat{N_i}(S_1 \cap S_2) = \hat{{\alpha}_i}\hat{N}(S_i),
     \\
    \hat{N_i}(S_1 \cup S_2) = \hat{{\beta}_i}\hat{N}(S_i)
\end{flalign}
Są to estymatory zgodne dla, odpowiednio, $|A_1 \cap A_2|$ oraz $|A_1 \cup A_2|$ \cite{ting}.

\section{Wariancja estymatorów składowych}

Zgodnie z formułą (\ref{wazony}), zastosowanie metody estymatora ważonego wymaga znajomości wariancji  estymatorów składowych lub przynajmniej jej dobrego przybliżenia.
Zauważmy, że jeśli warunki \textit{Centralnego Twierdzenia Granicznego} zachodzą dla estymatora $\hat{N}(S_i)$ to zbiega on według rozkładu do rozkładu normalnego. Wówczas stosując twierdzenie Slutsky'ego dla $\hat{\alpha}_i$ oraz $\hat{N}(S_i)$
możemy zapisać \cite{ting}:
%JL
%%\subsubsection{Centralne Twierdzenie Graniczne}
%%
%%Centralne Twierdzenie Graniczne jest twierdzeniem rachunku prawdopodobieństwa, za pomocą którego możemy w wielu sytuacjach założyć, że zmienna losowa, którą modelujemy dane zjawisko, ma rozkład mocno zbliżony do rozkładu normalnego.
%%\newline
%%Niech ${X_1, ..., X_n}$ będzie losowa próbą rozmiaru $n$ - t.j. sekwencją niezależnych zmiennych losowych o jednakowym rozkładzie prawdopodobieństwa o wartości oczekiwanej $\mu$ i skończonej wariancji $\sigma^2$. Ustalmy $S_n =  \frac{X_1 + ... + X_n}{n}$ jako średnią naszej próby. Wówczas gdy $n \rightarrow \infty$, zmienna losowa postaci $Z = \sqrt{n}(S_n - \mu)$ jest zbieżna według rozkładu do rozkładu normalnego $N(0, \sigma^2)$. Przydatność twierdzenia wynika z faktu, iż zmienna $Z$ dąży do rozkładu normalnego niezależnie od rozkładu poszczególnych zmiennych losowych $X_i$.
%%\newline 
\begin{equation}
    \sqrt{k_i}(\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i}) \rightarrow N(0, \sigma^2),
\end{equation}
gdzie wartość $\sigma^2$ jest nieznana.
Zauważmy również, że analogiczna zbieżność zachodzi dla  $\hat{{\beta}_i}\hat{N}(S_i)$).

Aby wyliczyć wariancję estymatora składowego zapiszmy:
\begin{equation}
\label{decomp}
    \sqrt{k_i}(\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i}) = \sqrt{k_i}\hat{{\alpha}_i}(\hat{N}(S_i) - n_{i}) + \sqrt{k_i}(\hat{{\alpha}_i} - {\alpha}_i)n_i
\end{equation}
i skorzystajmy ze wzoru $Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)$. Kowariancja między składniami sumy we wzorze (\ref{decomp}) jest  jednak  pomijalnie mała \cite{ting}, 
mamy zatem
\begin{flalign}
    Var(\sqrt{k_i}(\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i}))=
    \\
    Var(\sqrt{k_i}\hat{{\alpha}_i}(\hat{N}(S_i) - n_{i}) + \sqrt{k_i}(\hat{{\alpha}_i} - {\alpha}_i)n_i) \approx
    \\
    Var(\sqrt{k_i}\hat{{\alpha}_i}(\hat{N}(S_i) - n_{i})) + Var(\sqrt{k_i}(\hat{{\alpha}_i} - {\alpha}_i)n_i) =
    \\
    k_{i}{\hat{{\alpha}_i}}^{2}Var(\hat{N}(S_i) - n_{i}) + k_{i}{\hat{n_i}}^{2}Var(\hat{{\alpha}_i} - {\alpha}_i) =
    \\
    k_{i}{\hat{{\alpha}_i}}^{2}Var((\hat{N}(S_i)) + k_{i}{\hat{n_i}}^{2}Var(\hat{{\alpha}_i}).
\end{flalign}
Ostatecznie dla $  \hat{N_i}(S_1 \cap S_2) = \hat{{\alpha}_i}\hat{N}(S_i)$ otrzymujemy:
\begin{flalign}
    Var(\sqrt{k_i}(\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i})) \approx k_{i}{\hat{{\alpha}_i}}^{2}Var((\hat{N}(S_i)) + k_{i}{\hat{n_i}}^{2}Var(\hat{{\alpha}_i}),
    \\
    {k}_{i}Var((\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i})) \approx k_{i}{\hat{{\alpha}_i}}^{2}Var((\hat{N}(S_i)) + k_{i}{\hat{n_i}}^{2}Var(\hat{{\alpha}_i}),
    \\
    Var((\hat{{\alpha}_i}\hat{N}(S_i))) \approx {\hat{{\alpha}_i}}^{2}Var((\hat{N}(S_i)) + {\hat{n_i}}^{2}Var(\hat{{\alpha}_i}).
  \end{flalign}

Analogiczne wyprowadzenie można pokazać dla estymacji sumy estymatorów    $\hat{N_i}(S_1 \cup S_2) = \hat{{\beta}_i}\hat{N}(S_i)$.
Ponieważ wariancja dla $\hat{N}(S_i)$ jest nam znana \cite{streamed}, musimy jedynie określić wariancje  $Var(\hat{{\alpha}_i})$ oraz $Var(\hat{{\beta}_i})$. Autor \cite{ting} sugeruje wykorzystanie następujących formuł, choć nie wyjaśnia ich pochodzenia:
\begin{flalign}
    Var(\hat{{\alpha}_i}) \approx \frac{{\hat{{\alpha}_i}}(1 - \hat{{\alpha}_i})}{|h(S_i, {\tau}_{min})|},
\\
    Var(\hat{{\beta}_i}) \approx \frac{\hat{\beta}_i(\hat{\beta}_i - 1)}{|h(S_i, {\tau}_{min})|}.
\end{flalign}
Wzór na $Var(\hat{{\alpha}_i})$ można intuicyjnie wytłumaczyć w następujący sposób. Estymator $\hat{{\alpha}_i}$ jest w istocie serią $|h(S_i, {\tau}_{min})|$ niezależnych prób \textit{Bernoulliego}, gdzie z prawdopodobieństwem ${\alpha}_i$ losujemy element należący do przekroju $A_1 \cap A_2$. Prawdopodobieństwo zdarzenia przeciwnego wynosi $1 - {\alpha}_i$.
Oznaczając pojedynczą próbę jako $X_i$, oraz przyjmując $z = |h(S_i, {\tau}_{min})|$ możemy zapisać:
\begin{flalign}
Var(\hat{{\alpha}_i}) =
    Var(\frac{1}{z}\sum_{i=1}^{z}X_i) = 
%%    \\
%%    \frac{1}{z^2}Var(\sum_{i=1}^{z}X_i) = 
%%    \\
    \frac{1}{z^2}\sum_{i=1}^{z}Var(X_i) =
    \frac{1}{z^2}\sum_{i=1}^{z}{\alpha}_i(1 - {\alpha}_i) =
    \frac{{\alpha}_i(1 - {\alpha}_i)}{z}.
\end{flalign}

Wyprowadzenie wzoru na przybliżenie  $Var(\hat{{\beta}_i}) $ nie wydaje się już takie oczywiste. Rozumowanie oparte na technice zwanej \textit{delta metodą} zaproponujemy w kolejnym podrozdziale związanym z algorytmem \texttt{HyperLogLog}.


Powyższe przybliżenia wariancji możemy  zastosować do wzoru (4.19) co daje nam  przybliżenie wariancji $  \hat{N_i}(S_1 \cap S_2) = \hat{{\alpha}_i}\hat{N}(S_i)$:

\begin{flalign}
        Var(\hat{{\alpha}_i}\hat{N}(S_i)) \approx {\hat{{\alpha}_i}}^{2}\frac{{\hat{n_i}}^{2}}{2k_i} + {\hat{n_i}}^{2}\frac{\hat{{\alpha}_i}(1 - \hat{{\alpha}_i})}{|h(S_i, {\tau}_{min})|} =
    %%%%%    {\hat{n_i}}^{2}({\hat{{\alpha}_i}}^{2}\frac{1}{2k_i} + \frac{\hat{{\alpha}_i}^2(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}|h(S_i, {\tau}_{min})|})
%%%%%    \\
    ({\hat{n_i}\hat{{\alpha}_i}})^{2}(\frac{1}{2k_i} + \frac{(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}|h(S_i, {\tau}_{min})|}),
\end{flalign}
gdzie $\hat{n_i} = ... $ \JL{wyjasnic czym jest $\hat{n_i}$ i skad sie wzielo przyblizenie wariancji dla estymatora licznosci}
Ostatecznie 
\begin{equation}
    \hat{Var}(\hat{N_i}(S_1 \cap S_2)) \approx \hat{N_i}(S_1 \cap S_2)^{2}(\frac{1}{2{k}_i} + \frac{(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}|h(S_i, {\tau}_{min}|}).
\end{equation}

Wyprowadzenie dla $\hat{N_i}(S_1 \cup S_2)$ wygląda analogicznie:
\begin{flalign}
    \hat{Var}(\hat{N_i}(S_1 \cup S_2)) \approx \hat{N_i}(S_1 \cup S_2)^{2}(\frac{(\hat{{\beta}_i} - 1)}{\hat{{\beta}_i}|h(S_i, {\tau}_{min}|} + \frac{1}{2{k}_i}).
\end{flalign}

Mamy zatem wszystkie informacje potrzebne, by zastosować estymator ważony w postaci (4.1) dla sumy i przekroju w oparciu o 
szkice algorytmu \texttt{MinCount}.
Właściwości statystyczne tego estymatora w porównaniu do estymatorów naiwnych, zweryfikujmy empirycznie w ostatnim rozdziale pracy.

\JL{Tutaj skonczylem}
\section{Aproksymacja różnicy zbiorów metodą estymatora ważonego}

Jednym z otwartych tematów w pracy \cite{ting} jest wyprowadzenie estymatora składowego dla operacji różnicy zbiorów. W tym krótkim podrozdziale zdefiniujemy taki estymator, który następnie możemy wykorzystać w metodzie \textit{estymatora ważonego} do aproksymacji różnicy dwóch szkiców $\hat{N_i}(S_1 \setminus S_2)$.
\newline
Zauważmy, że różnicę dwóch zbiorów $A_1$ i $A_2$ możemy przedstawić jako przekrój pierwszego zbioru z dopełnieniem drugiego z nich:
\begin{equation}
    A_1 \setminus A_2 = A_1 \cap {A_2}^{c}.
\end{equation}
A co za tym idzie, moc różnicy dwóch zbiorów możemy przedstawić jako:
\begin{equation}
    |A_1 \setminus A_2| = \frac{|A_1 \setminus A_2|}{|A_i|}|A_i| = \frac{|A_1 \cap {A_2}^{c}|}{|A_i|}|A_i|.
\end{equation}

Zdefiniujmy zatem ${\gamma}_{i} = \frac{|A_1 \cap {A_2}^{c}|}{|A_i|} = \frac{|A_1 \setminus A_2|}{|A_i|}$ oraz jego estymator:
\begin{equation}
    \hat{{\gamma}_i} = \frac{|h(S_1 \setminus S_2)|}{|h(S_i, {\tau}_{min}|}
\end{equation}

Zauważmy że całą nasza przestrzenią $\Omega$ jest po prostu suma haszy z $S_1$ oraz $S_2$, więc aby wyznaczyć $|h(S_1 \setminus S_2)|$ wystarczy że odrzucimy ze zbioru haszy $h(S_1)$ wszystkie hasze znajdujące się w $h(S_2)$. Korzystając z faktu $(4.34)$ możemy przybliżyć $Var(\hat{{\gamma}_i})$ takim samym wzorem jak dla $\hat{{\alpha}_i}$, czyli:
\begin{equation}
    Var(\hat{{\gamma}_i}) \approx \frac{{\gamma}_i(1 - {\gamma}_i)}{|h(S_i, {\tau}_{min})|}
\end{equation}

Ostatecznie, przeprowadzając analogiczne obliczenia jak w $(4.31)$, otrzymujemy wzór na wariancję estymatora różnicy postaci:
\begin{equation}
    \hat{Var}(\hat{N_i}(S_1 \setminus S_2)) \approx \hat{N_i}(S_1 \setminus S_2)^{2}(\frac{1}{2{k}_i} + \frac{(1 - \hat{{\gamma}_i})}{\hat{{\gamma}_i}|h(S_i, {\tau}_{min}|})
\end{equation}


\section{Metoda estymatora ważonego dla algorytmu HyperLogLog}

Jak dotąd omówiliśmy zastosowanie metody \textit{estymatora ważonego} w kontekście algorytmu \texttt{MinCount}. W tym podrozdziale przedstawimy pomysł generalizacji tej metody na szkice  algorytmu \texttt{HyperLogLog}.
\newline
Potrzebne będą nam do tego estymatory mnożników ${\alpha}_i$ oraz ${\beta}_i$. Niestety nie jesteśmy w stanie policzyć ich korzystając bezpośrednio z informacji przechowywanych w szkicu HyperLogLog-a. Zauważmy jednak że zarówno ${\alpha}_i$ jak i ${\beta}_i$ możemy wyrazić za pomocą podobieństwa \textit{Jaccarda}:

\begin{flalign}
    {\alpha}_{i} = \frac{|A_1 \cap A_2|}{|A_i|} = J(A_1 \cap A_2, A_i)
    \\
    {\beta}_{i} = \frac{|A_1 \cup A_2|}{|A_i|} = J(A_i, A_1 \cup A_2,)^{-1}
\end{flalign}

Zatem aby znaleźć estymatory dla ${\alpha}_i$ i ${\beta}_i$, możemy posłużyć się estymatorami dla odpowiadających im indeksów \textit{Jaccarda}.


\JL{zakomentowałem Jaccarda i minhash i przenioslem na poczatek}
%
%gdzie podobieństwo \textit{Jaccarda} definiujemy jako:
%\begin{equation}
%    J(A, B) = \frac{|A \cap B|}{|A \cup B|}
%\end{equation}
%Definicję tę możemy także rozwinąć na $n$ zbiorów \cite{adroll}:
%\begin{equation}
%    J(A_1, A_2, ..., A_n) = \frac{|A_1 \cap A_2 \cap ... \cap A_n|}{|A_1 \cup A_2 \cup ... \cup A_n|}
%\end{equation}
%Zatem aby znaleźć estymatory dla ${\alpha}_i$ i ${\beta}_i$, możemy posłużyć się estymatorami dla odpowiadających im indeksów \textit{Jaccarda}. W tej pracy zaproponujemy wykorzystanie do tego, jako narzędzia pomocniczego - algorytmu \texttt{MinHash}, który wykorzystywany jest m.in. do estymacji podobieństwa \textit{Jaccarda} dwóch zbiorów danych.
%
%\subsubsection{Algorytm MinHash}
%W tym podrozdziale przypomnimy krótko działanie algorytmu \texttt{MinHash} oraz wyjaśnimy dlaczego możemy go zastosować w naszej metodzie estymacji.
%\newline
%Algorytm \texttt{MinHash} korzysta z techniki tzw. \textit{min-wise hashing}. Schemat ten został po raz pierwszy zaproponowany przez Andrei Brodera \cite{broder}, jako narzędzie do określania podobieństwa stron internetowych.
%
%Niech $h$ będzie funkcją haszującą, która mapuje elementy ze zbiorów $A$ i $B$ na liczby całkowite. Dla dowolnego zbioru $S$ zdefiniujmy $h_{min}(S)$ jako minimalny element z $S$ względem funkcji $h$ - to jest taki element $x \in S$ dla którego $h(x)$ osiąga najmniejszą wartość.
%
%Teraz, jeśli zastosujemy $h_{min}$ na obydwu zbiorach $A$ i $B$, zakładając że nie wystąpią żadne kolizje haszy - otrzymamy dokładnie tę samą wartość jeśli element $x$ należący do sumy zbiorów $A \cup B$ osiągający minimalną wartość $h(x)$ znajduje się także w przecięciu tych zbiorów $A \cap B$. Prawdopodobieństwo zajścia takiego zdarzenia jest równe właśnie indeksowi \textit{Jaccarda} tych zbiorów:
%\begin{equation}
%    Pr(h_{min}(A) = h_{min}(B)) = J(A, B)
%\end{equation}
%Inaczej mówiąc - jeśli $Y$ jest zmienną losową przyjmującą wartość $1$ gdy $h_{min}(A) = h_{min}(B)$ i $0$ w przeciwnym przypadku - wówczas $Y$ jest nieobciążonym estymatorem dla $J(A, B)$ \cite{minhash}.
%
%Ponieważ $Y$ ma zbyt dużą wariancję aby być przydatnym estymatorem indeksu \textit{Jaccarda} (ponieważ przyjmuje tylko wartość 0 lub 1), głównym pomysłem algorytmu \texttt{MinHash} jest zmniejszenie tej wariancji poprzez użycie estymatora będącego średnią z wielu takich zmiennych losowych. Najprostsza wersja algorytmu zakłada użycie $k$ różnych funkcji haszujących, gdzie $k$ to ustalony parametr będący liczbą całkowitą, a każdy zbiór $S$ jest reprezentowany przez $k$ wartości $h_{min}(S)$ policzonych przez $k$ funkcji haszujących.
%Estymator $J(A, B)$ w tej wersji algorytmu wygląda następująco:
%\begin{equation}
%    \hat{J}(A, B) = \frac{1}{k}\sum_{i=1}^{k}[h_{min}^{(i)}(A) = h_{min}^{(i)}(B)]
%\end{equation}
%gdzie $[x]$ jest notacją \textit{Iversona} dla zdarzenia $x$. Tak zdefiniowany estymator jest nieobciążonym estymatorem $J(A, B)$ \cite{minhash}, a jego wariancja wynosi:
%\begin{equation}
%    Var(\hat{J}(A,B)) = \frac{J(A, B)(1 - J(A, B))}{k}
%\end{equation}
%Ten wzór jest łatwo wyprowadzalny w podobny sposób jak wzór $(4.24)$. Estymator ten jest zmienną losową będącą sumą $k$ niezależnych prób \textit{Bernoulliego} postaci $[h_{min}^{(i)}(A) = h_{min}^{(i)}(B)]$, a jak wiemy ze wzoru $(4.38)$ prawdopodobieństwo sukcesu takiej jednej próby jest równe $J(A, B)$.
%
%Istnieje również drugi wariant tego algorytmu, który będziemy wykorzystywać w dalszej części tego rozdziału. Przedstawimy go od razu dla indeksu \textit{Jaccarda} dla wielu zbiorów $(4.38)$. Używa on analogicznego estymatora jak $(4.39)$, ale korzysta z tylko jednej funkcji haszującej. Zamiast generować $k$ różnych funkcji haszujących, przechowujemy $k$ najmniejszych wartości dla każdego z porównywanych zbiorów $A_1, A_2, ..., A_n$ i korzystamy z tylko jednej funkcji haszującej (analogicznie jak w algorytmie \texttt{MinCount}) \cite{adroll}. Posiadamy zatem zbiór haszy, który możemy traktować jako losową próbkę z $\bigcup A_i$. Następnie dla $k$ najmniejszych wartości z tej próbki zliczamy ile z nich znajduje się również wśród $k$ najmniejszych wartości w próbkach poszczególnych zbiorów. Całość dzielimy przez $k$ i otrzymujemy alternatywną wersję estymatora indeksu \textit{Jaccarda} \cite{adroll}:
%\begin{equation}
%    \hat{J}(A_1, A_2, ..., A_n) = \frac{|min_{k}\{\bigcup min_{k} \{ H(A_i) \} \} \cap (\bigcap min_{k}\{H(A_i)\})|}{k}
%\end{equation}
%gdzie $H(A_i)$ oznacza zbiór zhaszowanych elementów zbioru $A_i$, a $min_{k}\{S\}$ jest funkcją zwracającą $k$ najmniejszych elementów ze zbioru $S$ (w naszym przypadku są to hasze).
%
\subsubsection{Zastosowanie w algorytmie HyperLogLog}

Pomysłem zaproponowanym w tej pracy jest wykorzystanie algorytmu \texttt{MinHash} do policzenia estymatorów mnożników ${\alpha}_i$ oraz ${\beta}_i$, t.j:
\begin{flalign}
    \hat{{\alpha}_{i}} = \hat{J}(A_1 \cap A_2, A_i),
    \\
    \hat{{\beta}_{i}} = \hat{J}(A_i, A_1 \cup A_2,)^{-1}.
\end{flalign}

Ich wariancję możemy przybliżyć używając następujących wzorów. Dla $\hat{{\alpha}_i}$ będzie to po prostu wariancja estymatora $\hat{J}(A_1 \cap A_2, A_i)$, którą znamy dla algorytmu \texttt{MinHash}:
\begin{equation}
    Var(\hat{{\alpha}_i}) = \frac{{\alpha}_i(1 - {\alpha}_i)}{k} \approx \frac{\hat{J}(A_1 \cap A_2, A_i)(1 - \hat{J}(A_1 \cap A_2, A_i))}{k} 
\end{equation}

Wyznaczenie wariancji dla $\hat{{\beta}_i}$ nie jest już zupełnie trywialne, ponieważ estymator ten jest odwrotnością indeksu \textit{Jaccarda}. Skorzystamy zatem z techniki znanej czasem jako \textit{delta metoda} \JL{(zobacz: cytowanie pracy ''On Delta-Method of Moments and Probabilistic Sums'' J.Cichonia)},
 pozwalającej na oszacowanie wariancji funkcji zmiennej losowej, przy odpowiednich założeniach.
\begin{equation}
    Var(Y) \approx Var(f(X)) = (f'(E[X]))^{2}Var(X)
\end{equation}
Oznaczmy przez $b_i = \frac{1}{{\beta}_i} = J(A_i, A_1 \cup A_2)$, oraz niech $f(x) = \frac{1}{x}$. Wówczas ${{\beta}_i} = f(b_i)$. Podstawiając do wzoru $(4.43)$ otrzymujemy:
\begin{flalign}
    Var(\hat{{\beta}_{i}}) \approx Var(\frac{1}{{b_i}}) = ((\frac{1}{{b_i}})')^{2}Var({b_i}) =
    \\
    (-\frac{1}{{b_i}^2})^{2}\frac{{b_i}(1 - {b_i})}{k} =
    \\
    \frac{1}{{b_i}^3}\frac{(1 - {b_i})}{k} = 
    \\
    {{{\beta}_i}^3}\frac{(1 - \frac{1}{{\beta}_i})}{k} = 
    \\
    \frac{{{{\beta}_i}^2}({\beta}_i - 1)}{k} ,
\end{flalign}

\begin{equation}
    Var(\hat{{\beta}_{i}}) \approx \frac{{{{\beta}_i}^2}({\beta}_i - 1)}{k} \approx \frac{\hat{J}(A_i, A_1 \cup A_2)^{2}(\hat{J}(A_i, A_1 \cup A_2) - 1)}{k}.
\end{equation}

Aby zastosować schemat \textit{estymatora ważonego} potrzebna nam będzie jeszcze wariancja estymatora ${{\hat{N}}_i}^{HLL}(S)$ wyliczonego przez algorytm \texttt{HyperLogLog}. Jak przedstawiliśmy we wzorze $(2.12)$, wariancja estymatora wynosi:
\begin{equation}
    Var[{{\hat{N}}_i}^{HLL}(S)] = (n(\frac{{b}_m}{\sqrt{m}} + {\delta}_2(n) + o(1)))^2
\end{equation}

Ponieważ ${{\hat{N}}_i}^{HLL}(S)$ jest estymatorem asymptotycznie nieobciążonym \cite{hll}, a czynnik ${\delta}_2(n)$ jest pomijalnie mały - wariancję możemy przybliżyć takim wzorem:
\begin{equation}
    \hat{Var}[{{\hat{N}}_i}^{HLL}(S)] \approx ({{\hat{N}}_i}^{HLL}(S)(\frac{{b}_m}{\sqrt{m}}))^2
\end{equation}

Aby policzyć wariancję dla estymatorów składowych ${{\hat{N}}_i}^{HLL}(S_1 \cap S_2) = \hat{{\alpha}_i}{{\hat{N}}_i}^{HLL}(S_i)$ (i analogicznych dla sumy oraz różnicy), wykorzystamy wzór $(4.11)$. Obliczenia będą przebiegały analogicznie jak dla algorytmu \texttt{MinCount}, z tą różnicą, że teraz rozmiarem naszej próby nie jest już liczba przechowywanych haszy $k_i$, ale liczba rejestrów $m_i$:
\begin{flalign}
    Var((\hat{{\alpha}_i}\hat{N}^{HLL}(S_i))) \leq {\hat{{\alpha}_i}}^{2}({{\hat{n_i}\frac{b_{m}}{\sqrt{m}}}})^{2} + {\hat{n_i}}^{2}\frac{\hat{{\alpha}_i}(1 - \hat{{\alpha}_i})}{k} =
    \\
    {\hat{n_i}}^{2}({\hat{{\alpha}_i}}^{2}(\frac{b_{m}}{\sqrt{m}})^2 + \frac{\hat{{\alpha}_i}^2(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}k})
    \\
    {\hat{n_i}}^{2}{\hat{{\alpha}_i}}^{2}(\frac{b_{m}^2}{m} + \frac{(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}k})
\end{flalign}
\begin{equation}
    \hat{Var}(\hat{N_i}^{HLL}(S_1 \cap S_2)) \approx \hat{N_i}(S_1 \cap S_2)^{2}(\frac{b_{m}^2}{m} + \frac{(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}k})
\end{equation}
Obliczenia dla ${{\hat{N}}_i}^{HLL}(S_1 \cup S_2) = \hat{{\beta}_i}{{\hat{N}}_i}^{HLL}(S_i)$ wyglądają analogicznie. Ostateczny wzór ma postać:
\begin{equation}
    \hat{Var}(\hat{N_i}^{HLL}(S_1 \cup S_2)) \approx \hat{N_i}(S_1 \cup S_2)^{2}(\frac{b_{m}^2}{m} + \frac{(\hat{{\beta}_i} - 1)}{k})
\end{equation}

Tak wyznaczone przybliżenia wariancji możemy następnie użyć we wzorze na \textit{estymator ważony} $(4.1)$, analogicznie jak w przypadku algorytmu \texttt{MinCount}.
