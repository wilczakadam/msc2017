\chapter{Metoda estymatora ważonego}
\label{weighted_method}
\thispagestyle{chapterBeginStyle}




W tym rozdziale zajmiemy się metodą estymacji operacji teoriomnogościowych opisaną w \cite{ting}. Estymatory oparte na tej metodzie są definiowane jako średnia ważonej pewnej liczby  tzw. estymatorów składowych. Opiszemy w jaki sposób można zdefiniować estymatory składowe oraz jak jest optymalny sposób ich ważenia. 

\section{Optymalne ważenie estymatorów}
Na początku wyjaśnimy na czym polega ważenie estymatorów.
W tym celu, na początku przypomnimy definicje \textit{macierzy kowariancji}. Macierz ta jest uogólnieniem pojęcia wariancji na przypadek wielowymiarowy. Dla wektora zmiennych losowych $X_1, X_2, \ldots, X_n$ macierz kowariancji postać:
\[
\Sigma =
  \begin{bmatrix}
    {{\sigma}_1}^2 & {\sigma}_{12} & \cdots & {\sigma}_{1n}  \\
    {{\sigma}_{21}} & {{\sigma}_{2}}^2 & \cdots & {\sigma}_{2n} \\
    \vdots & \cdots & \ddots & \vdots \\
    {{\sigma}_{n1}} & {{\sigma}_{n2}} & \cdots & {{\sigma}_{n}^2}
  \end{bmatrix},
\]
gdzie
    ${{\sigma}_i}^2 = Var(X_i)$,
    ${\sigma}_{ij} = Cov(X_i, X_j)$.

%Warto dodać, że macierz kowariancji $\Sigma$ jest macierzą symetryczną, a jej wyznacznik jest nieujemny (jeżeli wektor zmiennych losowych jest niezdegenerowany). \JL{czy powyższa uwaga jest do czegokolwiek przydatna, a jesli tak to co to znaczy wektor niezdegenerowany?} 
\AW{Usunałem tę uwagę, chyba faktycznie nic nie wnosi}

Przedstawimy najpierw ogólną postać \textit{estymatora ważonego}, którego analizą zajemiemy się w tym podrozdziale. Załóżmy, że posiadamy ciąg zgodnych estymatorów $\hat{N_1}, ..., \hat{N_n}$ jakiegoś szkicu $S$. \textit{Estymatorem ważonym} $\hat{N}_w$ nazywac będziemy estymator powstały przez zsumowanie estymatorów składowych przemnożonych przez nadane im wagi:
\begin{equation}
\label{weighted_est_general}
\hat{N}_w = \sum_{i=1}^{n}w_i\hat{N_i}(S).
\end{equation}
Naszym celem jest znalezienie takich wartości $w_i$, które minimalizują wariancję estymatora $\hat{N}_w$. W tym celu skorzystamy z lematu przedstawionego w pracy \cite{ting}.
Poniżej prezentujemy treść tego lematu:
\begin{lemma}
	 Posiadając ciąg zgodnych estymatorów $\hat{N_1}, ..., \hat{N_n}$ szkicu $S$ o nieosobliwej macierzy kowariancji $\Sigma$, optymalne wagi które sumują się do $1$ i minimalizują wariancję estymatora $\hat{N}_w$ dane są przez:
	 \begin{equation}
	 	w_{opt} \propto  \Sigma^{-1}1_n
	 \end{equation}
	 gdzie $1_n$ to wektor jednostkowy długości $n$. Estymator $\hat{N}_w$ wykorzystujący wagi $w_{opt}$ jest nieobciążony (bądź zgodny) 
	 \JL{słowo bądź sugeruje, że jest albo tak albo tak, czy to miał Pan na mysli?}
	 \AW{Tak wynika z bezposredniego tlumaczenia z pracy Tinga - ''The resulting estimator is unbiased (or consistent)'' - tez mnie to zastanawialo, no ale tak jest tam napisane to uznalem ze przetlumacze wprost}
	 o wariancji ${{1_n}^{T}\Sigma^{-1}1_n}^{-1}$.
\end{lemma}
%\AW{Przepisalem ten akapit w formie lematu}
%\JL{Ta czesc pracy dalej nie ma sensu. W lemacie mowimy o minimalizowaniu wariancji, ale czego?
%Estymator ni jest jeszcze zdefiniowany? Wszystko nie układa sie w spojna całosc.}
\AW{Zmienilem aranzację tego podrozdzialu - mysle ze jest troche lepiej? Zmienilem tez troche tresc przytaczanego lematu zeby sie zgadzala z tym co dopisalem, mysle ze to nie jakis duzy grzech, zwlaszcza ze z oryginalnej wersji tez ciezko cos zrozumiec}
\JL{Jest OK, poza powyzszą uwagą o słowie 'bądz'}
W praktyce jednak macierz kowariancji $\Sigma$ rzadko jest znana i potrzebna jest jakaś jej aproksymacja.
Jeśli przyjemy, że estymatory składowe są niezależne
lub prawie niezależne, możemy poprzestać na wyliczeniu przekątnej macierzy kowariancji, czyli wariancji estymatorów składowych \cite{ting}. Wówczas naszymi wagami są $$w_i = \frac{1}{Var(\hat{N_i}(S))},$$ a zatem \textit{estymator ważony} $\hat{N}_w$, zgodnie ze wzorem \ref{weighted_est_general}, ma następującą postać:
\begin{equation}
\label{weighted_est}
    \hat{N}_w = \sum_{i=1}^{n}c\frac{\hat{N_i}(S)}{Var(\hat{N_i}(S))},
\end{equation}
%\JL{tzreba by jeszcze zdefinowac co znaczy $\hat{N_i}(S)$ ), bo to chyba nowe oznaczenie jest, czym jest S wyjasnic} 
%\AW{Jest teraz zdefiniowane w tresci lematu}
gdzie $c$ to stała normalizacyjna, zapewniająca, że wagi sumują się do jedynki.

\section{Estymatory składowe dla algorytmu \texttt{Streaming MinCount}}

W dalszej części tego rozdziału, dotyczącej szkiców algorytmu \texttt{MinCount}, zakładamy użycie algorytmu \texttt{Streaming MinCount} opisanego w rozdziale \ref{smincount}.
Estymator w tej wersji algorytmu jest bardziej skoncentrowany, a ponadto, co okaże się istotne, jego rozkład może być aproksymowany przez rozkład normalny (patrz wykres \ref{fig:str_mincount_hist}). 

Omówimy teraz metodę  definiowania estymatorów składowych. Naszym celem jest zdefiniowanie estymatorów opartach na dostępnym szkicu, które są jak najmniej nawzajem ze sobą skorelowane, oraz takich, których wariancję jesteśmy w stanie efektywnie wyliczyć lub przybliżyć. Poniżej przedstawiamy pomysł z pracy \cite{ting} umożliwiający wyrażenie estymatora ważonego sumy i przekroju jako  liniowej kombinacją  estymatorów liczności opartych na szkicach \texttt{MinCount}.

Aby zdefiniować estymatory składowe dla sumy i dla przekroju  zauważmy, że zarówno liczność sumy dwóch zbiorów, jak i ich przekroju możemy zapisać jako stosunek liczności sumy (analogicznie - przekroju) do liczności jednego z tych zbiorów pomnożonej przez liczność tegoż zbioru. Dla
\begin{flalign}
       {\alpha}_{i} = \frac{|A_1 \cap A_2|}{|A_i|}, \\
    {\beta}_{i} = \frac{|A_1 \cup A_2|}{|A_i|}
\end{flalign}
mamy zatem:
\begin{flalign}
       |A_1 \cap A_2| = {\alpha}_{i}|A_i| = \frac{|A_1 \cap A_2|}{|A_i|}|A_i|, \\
    |A_1 \cup A_2| = {\beta}_{i}|A_i| = \frac{|A_1 \cup A_2|}{|A_i|}|A_i|.
\end{flalign}
 Naturalnymi estymatorami dla $\alpha_i$ oraz $\beta_i$ zdefiniowanymi w oparciu o szkice \texttt{MinCount} wydają się być:
\begin{flalign}
     \hat{{\alpha}_{i}} = \frac{|h(S_1 \dot{\cap} S_2)|}{|h(S_i, {\tau}_{min})|},\\
     \label{var_alpha}
    \hat{{\beta}_{i}} = \frac{|h(S_1 \dot{\cup} S_2)|}{|h(S_i, {\tau}_{min})|}.
\end{flalign}
Zauważmy, że $\alpha_i$ oraz $\beta_i$ możemy interpretować jako stosunek mocy przekroju (i odpowiednio sumy) do mocy pojedynczego zbioru $A_i$. Ten stosunek estymujemy zatem korzystając z mocy zbiorów haszy dostępnych w naszych szkicach $S_1$ oraz $S_2$. $|h(S_1 \dot{\cap} S_2)|$ reprezentuje liczbę haszy w przekroju dwóch szkiców (zauważmy uzycie operacji $\dot{\cap}$ zdefiniowanej w (\ref{sketch-cut})), natomiast $|h(S_i, {\tau}_{min})|$ reprezentuje liczbę haszy w szkicu $S_i$. Stosunek tych dwóch wartości jest zatem estymatorem stosunku $\frac{|A_1 \cap A_2|}{|A_i|}$. Analogicznie zdefiniowany jest estymator dla $\beta_i$.
\AW{Proponuję takie wyjasnienie, ktore wydaje mi sie jest najbardziej sensowne}
\JL{Poprawic oznaczenia we wzorach, operacja sumy i przekroju na szkicach}
%\JL{zrobic odwolanie do wzoru \ref{sketch-cut} }
Powyższe estymatory są estymatorami zgodnymi \cite{ting}. 

Mając estymatory dla $\alpha_i$ i $\beta_i$ oraz estymatory liczności $\hat{N}(S_i)$ zbioru $A_i$ możemy zdefiniować estymatory składowe dla sumy i przekroju w następującej postaci:
\begin{flalign}
     \hat{N_i}(S_1 \dot{\cap} S_2) = \hat{{\alpha}_i}\hat{N}(S_i),
     \\
    \hat{N_i}(S_1 \dot{\cup} S_2) = \hat{{\beta}_i}\hat{N}(S_i)
\end{flalign}
Są to estymatory zgodne dla, odpowiednio, $|A_1 \cap A_2|$ oraz $|A_1 \cup A_2|$ \cite{ting}.

\section{Wariancja estymatorów składowych}

Zgodnie z formułą (\ref{weighted_est}), zastosowanie metody estymatora ważonego wymaga znajomości wariancji  estymatorów składowych lub przynajmniej jej dobrego przybliżenia.
Zauważmy, że jeśli warunki \textit{Centralnego Twierdzenia Granicznego} zachodzą dla estymatora $\hat{N}(S_i)$ to zbiega on według rozkładu do rozkładu normalnego. Wówczas stosując \textit{twierdzenie Slutsky'ego} dla $\hat{\alpha}_i$ oraz $\hat{N}(S_i)$
możemy zapisać \cite{ting}:
%JL
%%\subsubsection{Centralne Twierdzenie Graniczne}
%%
%%Centralne Twierdzenie Graniczne jest twierdzeniem rachunku prawdopodobieństwa, za pomocą którego możemy w wielu sytuacjach założyć, że zmienna losowa, którą modelujemy dane zjawisko, ma rozkład mocno zbliżony do rozkładu normalnego.
%%\newline
%%Niech ${X_1, ..., X_n}$ będzie losowa próbą rozmiaru $n$ - t.j. sekwencją niezależnych zmiennych losowych o jednakowym rozkładzie prawdopodobieństwa o wartości oczekiwanej $\mu$ i skończonej wariancji $\sigma^2$. Ustalmy $S_n =  \frac{X_1 + ... + X_n}{n}$ jako średnią naszej próby. Wówczas gdy $n \rightarrow \infty$, zmienna losowa postaci $Z = \sqrt{n}(S_n - \mu)$ jest zbieżna według rozkładu do rozkładu normalnego $N(0, \sigma^2)$. Przydatność twierdzenia wynika z faktu, iż zmienna $Z$ dąży do rozkładu normalnego niezależnie od rozkładu poszczególnych zmiennych losowych $X_i$.
%%\newline 
\begin{equation}
    \sqrt{k_i}(\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i}) \rightarrow N(0, \sigma^2),
\end{equation}
gdzie wartość $\sigma^2$ jest nieznana.
Zauważmy również, że analogiczna zbieżność zachodzi dla  $\hat{{\beta}_i}\hat{N}(S_i)$).

Aby wyliczyć wariancję estymatora składowego zapiszmy:
\begin{equation}
\label{decomp}
    \sqrt{k_i}(\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i}) = \sqrt{k_i}\hat{{\alpha}_i}(\hat{N}(S_i) - n_{i}) + \sqrt{k_i}(\hat{{\alpha}_i} - {\alpha}_i)n_i
\end{equation}
i skorzystajmy ze wzoru $Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)$. Kowariancja między składnikami sumy we wzorze (\ref{decomp}) jest  jednak  pomijalna \cite{ting}, 
mamy zatem
\begin{flalign}
    Var(\sqrt{k_i}(\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i}))=
    \\
    Var(\sqrt{k_i}\hat{{\alpha}_i}(\hat{N}(S_i) - n_{i}) + \sqrt{k_i}(\hat{{\alpha}_i} - {\alpha}_i)n_i) \approx
    \\
    Var(\sqrt{k_i}\hat{{\alpha}_i}(\hat{N}(S_i) - n_{i})) + Var(\sqrt{k_i}(\hat{{\alpha}_i} - {\alpha}_i)n_i) =
    \\
    k_{i}{\hat{{\alpha}_i}}^{2}Var(\hat{N}(S_i) - n_{i}) + k_{i}{\hat{n_i}}^{2}Var(\hat{{\alpha}_i} - {\alpha}_i) =
    \\
    k_{i}{\hat{{\alpha}_i}}^{2}Var((\hat{N}(S_i)) + k_{i}{\hat{n_i}}^{2}Var(\hat{{\alpha}_i}).
\end{flalign}
Ostatecznie dla $  \hat{N_i}(S_1 \dot{\cap} S_2) = \hat{{\alpha}_i}\hat{N}(S_i)$ otrzymujemy:
\begin{flalign}
    Var(\sqrt{k_i}(\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i})) \approx k_{i}{\hat{{\alpha}_i}}^{2}Var((\hat{N}(S_i)) + k_{i}{\hat{n_i}}^{2}Var(\hat{{\alpha}_i}),
    \\
    {k}_{i}Var((\hat{{\alpha}_i}\hat{N}(S_i) - n_{i}{\alpha}_{i})) \approx k_{i}{\hat{{\alpha}_i}}^{2}Var((\hat{N}(S_i)) + k_{i}{\hat{n_i}}^{2}Var(\hat{{\alpha}_i}),
    \\
    Var((\hat{{\alpha}_i}\hat{N}(S_i))) \approx {\hat{{\alpha}_i}}^{2}Var((\hat{N}(S_i)) + {\hat{n_i}}^{2}Var(\hat{{\alpha}_i}).
    \label{var_inter}
  \end{flalign}

Analogiczne wyprowadzenie można pokazać dla estymacji sumy szkiców $\hat{N_i}(S_1 \dot{\cup} S_2) = \hat{{\beta}_i}\hat{N}(S_i)$.
Ponieważ wariancja dla $\hat{N}(S_i)$ jest nam znana \cite{streamed}, musimy jedynie określić wariancje  $Var(\hat{{\alpha}_i})$ oraz $Var(\hat{{\beta}_i})$. Autor \cite{ting} sugeruje wykorzystanie następujących formuł, choć nie wyjaśnia ich pochodzenia:
\begin{flalign}
    Var(\hat{{\alpha}_i}) \approx \frac{{\hat{{\alpha}_i}}(1 - \hat{{\alpha}_i})}{|h(S_i, {\tau}_{min})|},
\\
    Var(\hat{{\beta}_i}) \approx \frac{\hat{\beta}_i(\hat{\beta}_i - 1)}{|h(S_i, {\tau}_{min})|}.
\end{flalign}
Wzór na $Var(\hat{{\alpha}_i})$ można intuicyjnie wytłumaczyć w następujący sposób. Estymator $\hat{{\alpha}_i}$ jest w istocie serią $|h(S_i, {\tau}_{min})|$ niezależnych prób \textit{Bernoulliego}, gdzie z prawdopodobieństwem ${\alpha}_i$ losujemy element należący do przekroju $A_1 \cap A_2$. Prawdopodobieństwo zdarzenia przeciwnego wynosi $1 - {\alpha}_i$.
Oznaczając pojedynczą próbę jako $X_i$, oraz przyjmując $z = |h(S_i, {\tau}_{min})|$ możemy zapisać:
\begin{flalign}
Var(\hat{{\alpha}_i}) =
    Var(\frac{1}{z}\sum_{i=1}^{z}X_i) = 
%%    \\
%%    \frac{1}{z^2}Var(\sum_{i=1}^{z}X_i) = 
%%    \\
    \frac{1}{z^2}\sum_{i=1}^{z}Var(X_i) =
    \frac{1}{z^2}\sum_{i=1}^{z}{\alpha}_i(1 - {\alpha}_i) =
    \frac{{\alpha}_i(1 - {\alpha}_i)}{z}.
\end{flalign}

Wyprowadzenie wzoru na przybliżenie  $Var(\hat{{\beta}_i}) $ nie wydaje się już takie oczywiste. Możemy otrzymać podobną formułę wykorzystując rozumowanie oparte na technice zwanej \textit{delta metodą}. Rozumowanie to opiszemy w kolejnym podrozdziale związanym z algorytmem \texttt{HyperLogLog}.


Powyższe przybliżenia wariancji możemy  zastosować do wzoru (\ref{var_inter}) co daje nam  przybliżenie wariancji $  \hat{N_i}(S_1 \dot{\cap} S_2) = \hat{{\alpha}_i}\hat{N}(S_i)$:

\begin{flalign}
        Var(\hat{{\alpha}_i}\hat{N}(S_i)) \approx {\hat{{\alpha}_i}}^{2}\frac{{\hat{n_i}}^{2}}{2k_i} + {\hat{n_i}}^{2}\frac{\hat{{\alpha}_i}(1 - \hat{{\alpha}_i})}{|h(S_i, {\tau}_{min})|} =
    %%%%%    {\hat{n_i}}^{2}({\hat{{\alpha}_i}}^{2}\frac{1}{2k_i} + \frac{\hat{{\alpha}_i}^2(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}|h(S_i, {\tau}_{min})|})
%%%%%    \\
    ({\hat{n_i}\hat{{\alpha}_i}})^{2}(\frac{1}{2k_i} + \frac{(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}|h(S_i, {\tau}_{min})|}),
\end{flalign}
gdzie $\hat{n_i}$ jest kroczącą estymatą liczności szkicu $S_i$ dla algorytmu \texttt{Streaming MinCount} przedstawioną wcześniej we wzorze (\ref{streaming_est}). Jego wariancja jest nam znana \cite{streamed} i wynosi:
$$Var(\hat{n_i}) = \frac{{{n}_i}^2}{2(k_i - 1)} \approx \frac{{\hat{n}_i}^2}{2k_i}.$$
\JL{wyjasnic czym jest $\hat{n_i}$ i skad sie wzielo przyblizenie wariancji dla estymatora licznosci}
\AW{".. skad sie wzielo przyblizenie wariancji dla estymatora licznosci" - ma Pan na mysli wariancję $\hat{n}_i$? - jesli tak to dodalem ją cytując zrodlo powyzej}
\JL{jest OK}
Ostatecznie:
\begin{equation}
    \hat{Var}(\hat{N_i}(S_1 \dot{\cap} S_2)) \approx \hat{N_i}(S_1 \dot{\cap} S_2)^{2}(\frac{1}{2{k}_i} + \frac{(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}|h(S_i, {\tau}_{min}|}).
    \label{var_sum_final}
\end{equation}
Wyprowadzenie dla $\hat{N_i}(S_1 \dot{\cup} S_2)$ wygląda analogicznie:
\begin{flalign}
    \hat{Var}(\hat{N_i}(S_1 \dot{\cup} S_2)) \approx \hat{N_i}(S_1 \cup S_2)^{2}(\frac{(\hat{{\beta}_i} - 1)}{\hat{{\beta}_i}|h(S_i, {\tau}_{min}|} + \frac{1}{2{k}_i}).
\end{flalign}

Mamy zatem wszystkie informacje potrzebne, by zastosować estymator ważony w postaci (\ref{weighted_est}) dla sumy i przekroju w oparciu o 
szkice algorytmu \texttt{Streaming MinCount}.
Właściwości statystyczne tego estymatora w porównaniu do estymatorów naiwnych, zweryfikujmy empirycznie w ostatnim rozdziale pracy.

\JL{Tutaj skonczylem}
\section{Aproksymacja różnicy zbiorów metodą estymatora ważonego}
\label{diff_weighted}

Jednym z otwartych tematów w pracy \cite{ting} jest wyprowadzenie estymatora dla operacji różnicy zbiorów w oparciu o ich szkice: $\hat{N_i}(S_1 \setminus S_2)$. W tym krótkim podrozdziale zdefiniujemy taki estymator, oparty na metodzie \textit{estymatora ważonego}.

Zauważmy po pierwsze, że różnicę dwóch zbiorów $A_1$ i $A_2$ możemy przedstawić jako przekrój pierwszego zbioru z dopełnieniem drugiego z nich:
\begin{equation}
    A_1 \setminus A_2 = A_1 \cap {A_2}^{c}.
\end{equation}
co niejako sprowadza nas do już rozwiązanego problemu estymacji przekroju zbiorów.
%%Przyjmijmy, że zbiory $A_1$ i $A_2$ stanowią całą przestrzeń, tzn. dla multizbioru $\mathfrak{M} = (S,m)$ mamy
%%$S=A_1 \cup A_2$. 
Analogicznie do wcześniejszego rozumowania  dla przekroju, moc różnicy dwóch zbiorów możemy przedstawić jako:
\begin{equation}
    |A_1 \setminus A_2| = \frac{|A_1 \setminus A_2|}{|A_i|}|A_i| = \frac{|A_1 \cap {A_2}^{c}|}{|A_i|}|A_i|.
    \label{diff_fact}
\end{equation}
Nieoczywistą kwestią może wydawać się wykorzystanie szkiców do estymacji rozmiaru dopełnienia zbioru.
Zauważmy, że moc $A_1 \cap A_2$ może być w przybliżeniu określona przez hasze $h(S_1) \cap h(S_2)$.
Aby estymować moc  $A_1 \cap A_2^c$ możemy zatem odrzucić ze zbioru haszy $h(S_1)$ hasze znajdujące się w $h(S_2)$.
Zdefiniujmy zatem mnożnik $${\gamma}_{i} = \frac{|A_1 \cap {A_2}^{c}|}{|A_i|}$$
% % % = \frac{|A_1 \setminus A_2|}{|A_i|}$ 
oraz jego estymator:
\begin{equation}
    \hat{{\gamma}_i} = \frac{|h(S_1 \dot{\setminus} S_2)|}{|h(S_i, {\tau}_{min})|},
\end{equation}
gdzie $S_1 \dot{\setminus} S_2$ oznacza operację na szkicach odrzucającą ze zbioru haszy $h(S_1)$ hasze znajdujące się w $h(S_2)$.
 Korzystając z faktu (\ref{diff_fact})
%\JL{cos chyba nie tak z numeracja, fakt 4.34?, najlepiej robic odniesnienia przez \ref{}}
możemy przybliżyć $Var(\hat{{\gamma}_i})$ takim samym wzorem jak dla $\hat{{\alpha}_i}$, czyli:
\begin{equation}
    Var(\hat{{\gamma}_i}) \approx \frac{{\hat{\gamma}}_i(1 - {\hat{\gamma}}_i)}{|h(S_i, {\tau}_{min})|}.
\end{equation}
Ostatecznie, przeprowadzając analogiczne obliczenia jak w (\ref{var_sum_final}),
 otrzymujemy wzór na wariancję estymatora różnicy postaci:
\begin{equation}
    \hat{Var}(\hat{N_i}(S_1 \dot{\setminus} S_2)) \approx \hat{N_i}(S_1 \dot{\setminus} S_2)^{2}(\frac{1}{2{k}_i} + \frac{(1 - \hat{{\gamma}_i})}{\hat{{\gamma}_i}|h(S_i, {\tau}_{min}|}).
\end{equation}
\JL{zmienić $\setminus$ nad $\dot{\setminus}$, choc głupio wyglada}


\section{Metoda estymatora ważonego dla algorytmu HyperLogLog}
\label{hll_weighted}

Jak dotąd omówiliśmy zastosowanie metody \textit{estymatora ważonego} w kontekście algorytmu \texttt{MinCount}. W tym podrozdziale przedstawimy nasz pomysł umożliwiający uogólnienia tej metody na szkice  algorytmu \texttt{HyperLogLog}.
Potrzebne będą nam do tego estymatory mnożników ${\alpha}_i$ oraz ${\beta}_i$. 
Niestety nie wydaje się oczywistym, jak można by je uzyskać korzystając bezpośrednio z informacji przechowywanych w szkicach algorytmu \texttt{HyperLogLog}. Zauważmy jednak że zarówno ${\alpha}_i$ jak i ${\beta}_i$ możemy wyrazić za pomocą podobieństwa \textit{Jaccarda}:

\begin{flalign}
    {\alpha}_{i} = \frac{|A_1 \cap A_2|}{|A_i|} = J(A_1 \cap A_2, A_i),
    \\
    {\beta}_{i} = \frac{|A_1 \cup A_2|}{|A_i|} = J(A_i, A_1 \cup A_2,)^{-1}.
\end{flalign}
Zatem, aby oszacować ${\alpha}_i$ oraz ${\beta}_i$, możemy posłużyć się estymacjami dla odpowiadających im indeksów \textit{Jaccarda}
\begin{flalign}
    \hat{{\alpha}_{i}} = \hat{J}(A_1 \cap A_2, A_i),
    \\
    \hat{{\beta}_{i}} = \hat{J}(A_i, A_1 \cup A_2,)^{-1},
\end{flalign}
które z kolei możemy łatwo uzyskać korzystając z algorytmu \texttt{MinHash} zaprezentowanego
w rozdziale \ref{minhash}. Zwróćmy, również uwagę, iż dzięki zastosowaniu techniki \textit{stochastycznego uśredniania}
algorytm \texttt{HyperLogLog} wykorzystuje jedynie jedną funkcję skrótu. W praktycznych zastosowaniach
użyteczne byłoby zatem oprzeć  również algorytm \texttt{MinHash} na jednej, a nie na $k$ niezależnych funkcjach skrótu.
Umożliwi nam to wersja tego algorytmu zaprezentowana na końcu rozdziału \ref{minhash}.
\AW{Tutaj użył Pan okreslenia ''funkcja skrótu'' natomiast w pozostalych miejscach w pracy uzywalismy dotychczas ''funkcja haszująca'' - czy powinienem przejsc wszedzie na ''funkcje skrótu''? Troche dziwnie brzmi, ale jesli to jest oficjalna naukowa nazwa to moze trzeba sie przerzucic :)}
\JL{Nie, to nie ma znaczenia żadnego :-) Może Pan zrobić, jak Pan uważa.}
\AW{ok, to zostane przy ''funkcji haszujacej''}

Wariancję powyższych estymatorów możemy przybliżyć używając następujących wzorów. Dla $\hat{{\alpha}_i}$ będzie to po prostu wariancja estymatora $\hat{J}(A_1 \cap A_2, A_i)$, którą znamy dla algorytmu \texttt{MinHash}:


\begin{equation}
    Var(\hat{{\alpha}_i}) = \frac{{\alpha}_i(1 - {\alpha}_i)}{k} \approx \frac{\hat{J}(A_1 \cap A_2, A_i)(1 - \hat{J}(A_1 \cap A_2, A_i))}{k} 
\end{equation}
Powyższy wzór wynika z rozumowania analogicznego do tego przedstawionego w wyprowadzeniu \ref{jacc_var2}.

Wyznaczenie wariancji dla $\hat{{\beta}_i}$ jest nieco trudniejsze, ponieważ estymator ten jest odwrotnością indeksu \textit{Jaccarda}. Skorzystamy zatem z techniki znanej czasem jako \textit{delta metoda}
(zobacz: \cite{cichon}),
 pozwalającej na oszacowanie wariancji funkcji zmiennej losowej, przy odpowiednich założeniach:
\begin{equation}
   Var(f(X)) \approx (f'(E[X]))^{2}Var(X).
   \label{delta_method}
\end{equation}
Oznaczmy przez $b_i = \frac{1}{{\hat{\beta}}_i} = \hat{J}(A_i, A_1 \cup A_2)$, oraz niech $f(x) = \frac{1}{x}$. Wówczas ${{\hat{\beta}}_i} = f(b_i)$. Podstawiając do wzoru (\ref{delta_method})
 otrzymujemy:
\begin{flalign}
    Var(\hat{{\beta}_{i}}) \approx  ((\frac{1}{{b_i}})')^{2}Var({b_i}) =
    \\
    (-\frac{1}{{b_i}^2})^{2}\frac{{b_i}(1 - {b_i})}{k} =
    \\
    \frac{1}{{b_i}^3}\frac{(1 - {b_i})}{k} = 
    \\
    {{{\hat{\beta}}_i}^3}\frac{(1 - \frac{1}{{\hat{\beta}}_i})}{k} = 
    \\
    \frac{{{{\hat{\beta}}_i}^2}({\hat{\beta}}_i - 1)}{k} ,
\end{flalign}

\begin{equation}
    Var(\hat{{\beta}_{i}}) \approx \frac{{{{\beta}_i}^2}({\beta}_i - 1)}{k} \approx \frac{\hat{J}(A_i, A_1 \cup A_2)^{2}(\hat{J}(A_i, A_1 \cup A_2) - 1)}{k}.
\end{equation}


Aby zastosować schemat \textit{estymatora ważonego} potrzebna nam będzie jeszcze wariancja estymatora ${{\hat{N}}_i}^{HLL}(S)$ wyliczonego przez algorytm \texttt{HyperLogLog}. Jak przedstawiliśmy we wzorze (\ref{hll_var}), wariancja estymatora wynosi:
\begin{equation}
    Var[{{\hat{N}}_i}^{HLL}(S)] = (n(\frac{{b}_m}{\sqrt{m}} + {\delta}_2(n) + o(1)))^2.
\end{equation}
Ponieważ ${{\hat{N}}_i}^{HLL}(S)$ jest estymatorem asymptotycznie prawie nieobciążonym \cite{hll}, a czynnik ${\delta}_2(n)$ jest pomijalnie mały, wariancję możemy przybliżyć wzorem:
\begin{equation}
    \hat{Var}[{{\hat{N}}_i}^{HLL}(S)] \approx ({{\hat{N}}_i}^{HLL}(S)(\frac{{b}_m}{\sqrt{m}}))^2.
\end{equation}


Aby policzyć wariancję dla estymatorów składowych przekroju ${{\hat{N}}_i}^{HLL}(S_1 \cap S_2) = \hat{{\alpha}_i}{{\hat{N}}_i}^{HLL}(S_i)$  wykorzystamy wzór (\ref{var_inter}).
Zauważmy, że estymator w \texttt{HyperLogLog} zbiega do rozkładu normalnego, 
patrz rozdział \ref{hll}.
 Obliczenia będą przebiegały analogicznie jak dla algorytmu \texttt{MinCount}, z tą różnicą, że teraz rozmiarem naszej próby nie jest już liczba przechowywanych haszy $k$, ale liczba rejestrów $m$:
\begin{flalign}
    Var((\hat{{\alpha}_i}\hat{N}^{HLL}(S_i))) \approx {\hat{{\alpha}_i}}^{2}({{\hat{n_i}\frac{b_{m}}{\sqrt{m}}}})^{2} + {\hat{n_i}}^{2}\frac{\hat{{\alpha}_i}(1 - \hat{{\alpha}_i})}{k} =
    \\
    {\hat{n_i}}^{2}({\hat{{\alpha}_i}}^{2}(\frac{b_{m}}{\sqrt{m}})^2 + \frac{\hat{{\alpha}_i}^2(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}k}) =
    \\
    {\hat{n_i}}^{2}{\hat{{\alpha}_i}}^{2}(\frac{b_{m}^2}{m} + \frac{(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}k}),
\end{flalign}
\begin{equation}
    \hat{Var}(\hat{N_i}^{HLL}(S_1 \cap S_2)) \approx \hat{N_i}(S_1 \cap S_2)^{2}(\frac{b_{m}^2}{m} + \frac{(1 - \hat{{\alpha}_i})}{\hat{{\alpha}_i}k}).
\end{equation}

Analogiczne obliczenia można przeprowadzić w przypadku estymatorów sumy oraz różnicy.
Dla estymacji sumy ${{\hat{N}}_i}^{HLL}(S_1 \cup S_2) = \hat{{\beta}_i}{{\hat{N}}_i}^{HLL}(S_i)$ otrzymujemy:
\begin{equation}
    \hat{Var}(\hat{N_i}^{HLL}(S_1 \cup S_2)) \approx \hat{N_i}(S_1 \cup S_2)^{2}(\frac{b_{m}^2}{m} + \frac{(\hat{{\beta}_i} - 1)}{k}).
\end{equation}
Tak wyznaczone przybliżenia wariancji możemy następnie użyć we wzorze na \textit{estymator ważony} (\ref{weighted_est}),
 analogicznie jak w przypadku algorytmu \texttt{MinCount}.


