\chapter{Operacje teoriomnogościowe}
\thispagestyle{chapterBeginStyle}

W tym rozdziale wprowadzimy pojęcie \textit{podobieństwa Jaccarda} zbiorów oraz przedstawimy algorytm \texttt{MinHash} służący do estymacji takiego podobieństwa zbiorów bez konieczności pamiętania wszystkich jego elementów.
 Następnie omówimy podstawowe metody estymacji liczności zbiorów powstałych w wyniku wykonania operacji teoriomnogościowych w przypadku, gdy nie są znane bezpośrednio zbiory na jakich wykonywane są te operacje, a jedynie ich szkice. 

\subsubsection{Podobieństwo Jaccarda}
Podobieństwo Jaccarda definiujemy jako:
\begin{equation}
    J(A, B) = \frac{|A \cap B|}{|A \cup B|}.
\end{equation}
Definicję tę możemy także uogólnić  na $n$ zbiorów \cite{adroll}:
\begin{equation}
    J(A_1, A_2, ..., A_n) = \frac{|A_1 \cap A_2 \cap ... \cap A_n|}{|A_1 \cup A_2 \cup ... \cup A_n|}.
    \label{jacc_multi}
\end{equation}
 W tej pracy zaproponujemy wykorzystanie do tego, jako narzędzia pomocniczego - algorytmu \texttt{MinHash}, który wykorzystywany jest m.in. do estymacji podobieństwa \textit{Jaccarda} dwóch zbiorów danych.

\subsubsection{Algorytm MinHash}
\label{minhash}
W tym podrozdziale opiszemy działanie algorytmu \texttt{MinHash} oraz wyjaśnimy w jaki sposób możemy go zastosować do estymacji \textit{podobieństwa Jaccarda} wbiorów.
Algorytm \texttt{MinHash} korzysta z techniki tzw. \textit{min-wise hashing}. Schemat ten został po raz pierwszy zaproponowany przez Andrei Brodera \cite{broder}, jako narzędzie do określania podobieństwa stron internetowych.

Niech $h$ będzie funkcją haszującą.
%., która mapuje elementy ze zbiorów na liczby.
 %całkowite. 
 \JL{czy to ma isitotne znaczenie, ze calkowite?}
 \AW{chyba niekoniecznie - przetlumaczylem doslownie z "integer"}
 \JL{Może tak jak teraz?}
 Dla dowolnego zbioru $X$ zdefiniujmy $h_{min}(X)$ jako minimalny element z $X$ względem funkcji $h$ , to jest taki element $x \in X$ dla którego $h(x)$ osiąga najmniejszą wartość.

Zastosujemy funkcję $h_{min}$ na zbiorach $A$ i $B$. Zakładając że nie wystąpią żadne kolizje haszy, otrzymamy dokładnie tę samą wartość, jeśli element $x$ należący do sumy zbiorów $A \cup B$ osiągający minimalną wartość $h(x)$ znajduje się także w przecięciu tych zbiorów $A \cap B$. Prawdopodobieństwo zajścia takiego zdarzenia jest równe właśnie indeksowi \textit{Jaccarda} tych zbiorów:
\begin{equation}
    Pr(h_{min}(A) = h_{min}(B)) = J(A, B)
    \label{jacc_prob}
\end{equation}
Łatwo zatem pokazać, że jeśli $I$ jest zmienną losową przyjmującą wartość $1$ gdy $h_{min}(A) = h_{min}(B)$ i $0$ w przeciwnym przypadku, wówczas $I$ jest nieobciążonym estymatorem dla $J(A, B)$ \cite{minhash}.

Ponieważ zmienna $I$ ma zbyt dużą wariancję, aby być przydatnym estymatorem indeksu \textit{Jaccarda}, główną ideą algorytmu \texttt{MinHash} jest zmniejszenie tej wariancji poprzez użycie estymatora będącego średnią z wielu niezależnych eksperymentów tego typu. Najprostsza wersja algorytmu zakłada użycie $k$ różnych funkcji haszujących, gdzie $k$ to ustalony parametr. Innymi słowy, każdy zbiór $X$ jest reprezentowany przez $k$ wartości $h^{(1)}_{min}(X), h^{(2)}_{min}(X), \ldots, h^{(k)}_{min}(X)$ policzonych przez $k$ funkcji haszujących.
Estymator $J(A, B)$ w tej wersji algorytmu wygląda następująco:
\begin{equation}
    \hat{J}(A, B) = \frac{1}{k}\sum_{i=1}^{k}[h_{min}^{(i)}(A) = h_{min}^{(i)}(B)],
    \label{jacc_est}
\end{equation}
gdzie $[x]$ jest notacją \textit{Iversona} dla zdarzenia $x$, zdefiniowaną jako 
$$[x] = \left\{ \begin{array}{rl}
	1 &\mbox{ jeśli zdarzenie $x$ jest prawdziwe, } \\
	0 &\mbox{ w p.p.}
\end{array} \right.$$ \JL{uzupelnić}
Tak zdefiniowany estymator jest nieobciążonym estymatorem $J(A, B)$ \cite{minhash}, a jego wariancja wynosi:
\begin{equation}
    Var(\hat{J}(A,B)) = \frac{J(A, B)(1 - J(A, B))}{k}
    \label{jacc_var}
\end{equation}
Ten wzór można łatwo wyprowadzić. Zauważmy, że estymator $\hat{J}(A, B)$ jest zmienną losową będącą sumą $k$ niezależnych prób \textit{Bernoulliego} postaci $[h_{min}^{(i)}(A) = h_{min}^{(i)}(B)]$, a jak wiemy ze wzoru (\ref{jacc_prob}) prawdopodobieństwo sukcesu takiej jednej próby jest równe $J(A, B)$. Oznaczmy przez $X_i = [h_{min}^{(i)}(A) = h_{min}^{(i)}(B)]$. Wówczas możemy zapisać:
\begin{equation}
	\hat{J}(A,B) = \frac{1}{k}\sum_{i=1}^{k}X_i
\end{equation}
Nakładając wariancję otrzymujemy:
\begin{flalign}
	Var(\frac{1}{k}\sum_{i=1}^{k}X_i) = 
	\\
	\frac{1}{k^2}Var(\sum_{i=1}^{k}X_i) = 
	\\
	\frac{1}{k^2}\sum_{i=1}^{k}Var(X_i) =
	\\
	\frac{1}{k^2}\sum_{i=1}^{k}J(A, B)(1 - J(A, B)) =
	\\
	\frac{J(A, B)(1 - J(A, B))}{k}
	\label{jacc_var2}
\end{flalign}


Istnieje również drugi, mniej znany wariant algorytmu \texttt{MinHash}, który będziemy wykorzystywać w dalszej części pracy.
 Używa on analogicznego estymatora jak \ref{jacc_est}, ale korzysta z tylko jednej funkcji haszującej. Zamiast generować $k$ różnych funkcji haszujących, przechowujemy $k$ najmniejszych wartości dla porównywanych zbiorów $A_1, A_2$ i korzystamy z tylko jednej funkcji haszującej (analogicznie jak w algorytmie \texttt{MinCount}) \cite{adroll}. Posiadamy zatem zbiór haszy, który możemy traktować jako losową próbkę z $\bigcup A_i$. Następnie dla $k$ najmniejszych wartości z tej próbki zliczamy ile z nich znajduje się również wśród $k$ najmniejszych wartości w próbkach poszczególnych zbiorów. Całość dzielimy przez $k$ i otrzymujemy alternatywną wersję estymatora indeksu \textit{Jaccarda} \cite{adroll}:

 \begin{equation}
 \hat{J}(A_1, A_2) = \frac{|min_{k}\{min_{k} \{ h(A_1) \} \cup min_{k} \{ h(A_2) \} \} \cap (min_{k}\{h(A_1)\} \cap min_{k}\{h(A_2)\})|}{k}
 \end{equation}
 gdzie $h(A_i)$ oznacza zbiór zhaszowanych elementów zbioru $A_i$, a $min_{k}\{S\}$ jest funkcją zwracającą $k$ najmniejszych elementów ze zbioru $S$ (w naszym przypadku są to hasze).
 Estymator ten możemy łatwo uogólnić dla \textit{indeksu Jaccarda} $n$ zbiorów, zdefiniowanego w (\ref{jacc_multi}):
\begin{equation}
    \hat{J}(A_1, A_2, ..., A_n) = \frac{|min_{k}\{\bigcup min_{k} \{ h(A_i) \} \} \cap (\bigcap min_{k}\{h(A_i)\})|}{k}
\end{equation}

\section{Naiwna estymacja operacji teoriomnogościowych}
\label{naive_est}

Omówimy teraz naiwne metody wyników estymacji operacji teoriomnogościowych na szkicach, skupimy się na operacji sumy i przekroju. Dla niektóre szkiców danych operacja sumy może być zdefiniowana w stosunkowo naturalny sposób. Dla przykładu, łatwo zauważyć, że  oszacowanie liczności dla sumy dwóch zbiorów $A_1$ i $A_2$ przy użyciu szkiców $M_1$ i $M_2$ związanych z algorytmem \texttt{HyperLogLog} sprowadza się do stworzenia nowego szkicu $M_{A_1\cup A_2}$, takiego, że 
$$M_{A_1\cup A_2}[i] = \max(M_{A_1}[i], M_{A_2}[i]).$$ 

Najważniejszą własnością powyższej operacji jest to, że powstały szkic jest identyczny ze szkicem, który powstałby z sumy zbiorów wejściowych $A_1\cup A_2$ oraz fakt, że suma dwóch szkiców daje w wyniku nowy szkic. Oznacza to że operacja sumy jest zamknięta i pozawala m.in. na sumowanie ze sobą sekwencyjnie większej liczby szkiców. Tę operację dla algorytmu \texttt{HyperLogLog} omówimy jeszcze dokładniej w podrozdziale \ref{HLL_sum}.
Bardziej formalnie, możemy zdefiniować taką operację $\dot{\cup}$, że dla dwóch dowolnych zbiorów $A, B$:
\begin{equation}
    S(A_1) \dot{\cup} S(A_2) = S(A_1 \cup A_2),
\end{equation}
\JL{zamienilem znaczek $\hat{\cup}$ na $\dot{\cup}$}
gdzie $S$ to funkcja generująca szkic danych dla zbioru.

W przeciwieństwie do operacji sumy, dla szkicach rozważanych przez nas algorytmów nie istnieje naturalna operacja przekroju. Istnieją metody umożliwiające
oszacowanie mocy przekroju, ale nie zwracają one wyniku nowego szkicu, tak jak operacja sumy. Jedną z takich podstawowych metod jest zastosowanie zasady \textit{włączeń i wyłączeń}: 
\begin{equation}
    |A_1 \cap A_2| = |A_1| + |A_2| - |A_1 \cup A_2|
    \label{incexc}
\end{equation}
i skorzystanie z wcześniej wyznaczonej estymacji sumy do policzenia estymacji przekroju.
Metodę tę można również wykorzystać do oszacowania różnicy zbiorów:
\begin{equation}
    \begin{aligned}
        |A_1 \setminus A_2| = |A_1 \cup A_2| - |A_2|.
    \end{aligned}
\end{equation}
Estymatę przekroju można również policzyć korzystając z podobieństwa \textit{Jaccarda}. Poniżej podajemy zestawienie estymatorów operacji sum i przekroju zdefiniowanych zgodnie z naiwnym podejściem opisanym powyżej:
\begin{flalign}
        \hat{N}(S_1 \hat{\cup} S_2) &= \hat{N}(S(A_1 \cup A_2))\\
        \hat{N}(S_1 \hat{\cap} S_2) &= \hat{N}(S(A_1)) + \hat{N}(S(A_2)) - \hat{N}(S(A_1 \cup A_2)), 
\end{flalign}
a także z wykorzystaniem podobieństwa \textit{Jaccarda}:

\begin{flalign}
\hat{N_1}(S_1 \hat{\cap} S_2) &= \hat{J}(S(A_1), S(A_2))\hat{N}(S_1 \hat{\cup} S_2) \label{jacc_1} \\
        \hat{N_2}(S_1 \hat{\cap} S_2) &= \frac{\hat{J}(S(A_1), S(A_2))}{1 + \hat{J}(S(A_1), S(A_2))}(\hat{N}(S(A_1)) + \hat{N}(S(A_2))). \label{jacc_2}
\end{flalign}
Wzór (\ref{jacc_1}) możemy uzasadnić korzystając z następującej własności:
\begin{equation}
	J(A_1, A_2)|A_1 \cup A_2| = \frac{|A_1 \cap A_2|}{|A_1 \cup A_2|}|A_1 \cup A_2| = |A_1 \cap A_2|.
\end{equation}
Natomiast wzór (\ref{jacc_2}) wynika z następujących przekształceń:
\begin{flalign}
&\frac{J(A_1, A_2)}{1 + J(A_1, A_2)}(|A_1| + |A_2|) \\
&= \frac{\frac{|A_1 \cap A_2|}{|A_1 \cup A_2|}}{1 + \frac{|A_1 \cap A_2|}{|A_1 \cup A_2|}}(|A_1| + |A_2|) \\
&= \frac{|A_1 \cap A_2|}{|A_1 \cup A_2|}\frac{|A_1 \cup A_2|}{|A_1 \cup A_2| + |A_1 \cap A_2|}(|A_1| + |A_2|) \\
&= \frac{|A_1 \cap A_2|}{|A_1 \cup A_2| + |A_1 \cap A_2|}|A_1| + \frac{|A_1 \cap A_2|}{|A_1 \cup A_2| + |A_1 \cap A_2|}|A_2| \\
&= \frac{|A_1 \cap A_2|(|A_1| + |A_2|)}{|A_1 \cup A_2| + |A_1 \cap A_2|}. \\
\end{flalign}
Następnie korzystając z faktu (\ref{incexc}) otrzymujemy:
\begin{equation}
	\frac{|A_1 \cap A_2|(|A_1| + |A_2|)}{|A_1 \cup A_2| + |A_1 \cap A_2|} = |A_1 \cap A_2|.
\end{equation}

Powyższe metody estymacji nie są jednak zbyt dokładne. Zwłaszcza w przypadku przekroju, gdzie błąd jest z mniej więcej proporcjonalny do wielkości sumy lub większego zbioru, a oczekiwalibyśmy błędu ograniczony rozmiarem mniejszego zbioru \cite{ting}. W skutek użycia powyższych formuł często dochodzi również do anomalii w wyniku których oszacowanie liczności jest ujemne. Dlatego w dalszej części pracy zajmiemy się analizą innych metod szacowania wyników operacji teoriomnogościowych, które dają dokładniejsze wyniki i pozbawione są tego rodzaju anomalii. Ponadto, przynajmniej w przypadku algorytmu \texttt{MinCount}, pozawalają na zdefiniowanie zamkniętej operacji przekroju.


% JL - chyba to jest niepotrzebne:
%%W tym rozdziale przyjrzymy się dokładniej naiwnym podejściom do aproksymacji operacji teoriomnogościowych dla szkiców danych zarówno dla algorytmu \texttt{MinCount} jak i \texttt{HyperLogLog}. Takie podejścia są łatwe w implementacji i nie wymagają żadnych modyfikacji istniejących algorytmów ale, zwłaszcza w przypadku przekroju i różnicy, są stosunkowo niedokładne, mogą prowadzić do anomalii oraz w przypadku niektórych algorytmów (np. \texttt{HyperLogLog}) nie pozwalają na zdefiniowanie zamkniętego operatora zwracającego nowy szkic a jedynie na estymację wyniku liczności.

\section{HyperLogLog}

\subsection{Operacja sumy}
\label{HLL_sum}

W poprzedniej sekcji wyjaśniliśmy, że algorytm \texttt{HyperLogLog} posiada naturalną, zamkniętą operację sumy teoriomnogościowej na swoich szkicach. Jest ona wyjątkowo nieskomplikowana i sprowadza się do znalezienia maksimum na każdym z rejestrów spośród sumowanych szkiców \cite{oertl}. Mając dwa szkice \texttt{HyperLogLog-a} rozmiaru $m$: $M_{A_1} = (M_{A_1}[1], ..., M_{A_1}[m])$ oraz $M_{A_2} = (M_{A_2}[1], ..., M_{A_2}[m])$ reprezentujące dwa zbiory $A_1$ oraz odpowiednio $A_2$, procedura tworząca szkic $M_{A_1 \cup A_2} = (M_{A_1 \cup A_2}[1], ..., M_{A_1 \cup A_2}[m])$ reprezentujący sumę $A_1 \cup A_2$ wygląda następująco:
\begin{equation}
    M_{A_1 \cup A_2} = \max(M_{A_1}[i], M_{A_2}[i]) \quad \textbf{dla} \quad i = 1, ..., m.
\end{equation}

Pamiętajmy, że w ten sposób możemy sumować szkice o tych samych parametrach $p$ i $q$ (patrz rozdział \ref{HLL_sketch}). Istnieje jednak możliwość sumowania dwóch szkiców o różnych parametrach parach $(p, q)$ oraz $(p',q')$, albowiem każdy szkic \texttt{HyperLogLog} opisany przez parę $(p, q)$ może zostać zredukowany do szkicu opisanego przez $(p', q')$, jeśli spełniony jest warunek $p' \leq p$ oraz $p' + q' \leq p + q$. Taka transformacja jest bezstratna, tj. powstały szkic jest taki sam jak szkic który powstałby przez dodawanie tych wszystkich elementów od początku do szkicu opisanego przez $(p', q')$ \cite{oertl}. Algorytm \ref{HLL_compress} przedstawia pseudokod procedury wykonującej taką transformację.
%\JL{opisac doklanie jak tak redukacja sie odbywa!
\AW{Czy wystarczy pseudokod z pracki w której go znalazłem? Nawet tam nie jest dokładnie opisane co tu się dzieje, więc nie wiem czy jest sens sie nad tym rozpisywac - ten fakt o kompresji chcialem tutaj podac tak bardziej informacyjnie zeby bylo wiadomo ze mozna sumowac szkice o roznych p i q}
\JL{Na razie, niech tak zostanie}
\begin{algorithm}
	\begin{algorithmic}
		\State $p, q, M $ - parametry oraz kolekcja rejestrów kompresowanego szkicu
		\State $p', q' $ - parametry wejściowe szkicu docelowego
		\State Założenie: $ p' \leq p$ oraz $p' + q' \leq p + q$
		\State $Q(s) $ -  funkcja zwracając pozycję pierwszej jedynki w ciągu bitów $s$ 
		\newline
		\Function{Compress}{$M$}
		\State $m' = 2^{p'}$
		\For {$i = 1 .. m'$}
			\State $M'[i] \gets 0$
		\EndFor
		\For {$i = 1 .. m'$}
			\State $b \gets (i - 1)2^{p-p'}$
			\State $j \gets 1$
			\While {$j \leq 2^{p-p'}$ and $M[b+j] = 0$}
				\State $j \gets j + 1$
			\EndWhile
			\If {$j = 1$}
				\State $M'[i] \gets \min(M[b+j] + (p - p'), q' + 1)$
			\ElsIf {$j \leq 2^{p-p'}$}
				\State ${{\langle}e_1, e_2, ..., e_{p-p'}{\rangle}}_2 \gets j - 1$
				\State $v \gets {{\langle}e_1, e_2, ..., e_{p-p'}{\rangle}}_2$ 
				\State $M'[i] \gets Q(v)$
			\Else
				\State $M'[i] = 0$
			\EndIf
		\EndFor
		\State \Return $M'$
		\EndFunction
		
	\end{algorithmic}
	\caption{Procedura kompresji szkicu \texttt{HyperLogLog}}
	\label{HLL_compress}
\end{algorithm}

\subsection{Operacja przekroju i różnicy}

Operacji sumy na szkicach algorytmu \texttt{HyperLogLog} sprawdza się całkiem dobrze i jest  łatwa w implementacji. Niestety nie są znane naturalne i zamknięte operacje przekroju oraz różnicy na tych szkicach. Oczywiście stosunkowo łatwo możemy estymować liczność przekroju zbiorów korzystając z zasady włączeń i wyłączeń lub podobieństwa Jaccarda.

O ile metoda oparta na zasadzie włączeń i wyłączeń nie sprawdza się w tym przypadku i prowadzi do anomalii, to metoda wykorzystująca podobieństwo \textit{Jaccarda} (\ref{jacc_1}) do estymacji przekroju jest stosunkowo dokładna, co potwierdzają wykonane przez nas eksperymenty. Wymaga ona jednak dodatkowej struktury danych pozwalającej na estymację indeksu \textit{Jaccarda} zbiorów. Częstą praktyką jest wykorzystanie do tego algorytmu \texttt{MinHash} \cite{adroll}. Wyniki związane z tym podejściem zostaną jeszcze omówione w kolejnym rozdziale.


\section{MinCount}
\label{impr_mincount}

\subsection{Operacja sumy}
\label{impr_sum}

Przyjrzyjmy się teraz operacji sumy dla algorytmu \texttt{MinCount}. Załóżmy, że posiadamy dwa szkice danych $S_1 = (H_1, {\tau}_1)$ oraz $S_2 = (H_2, {\tau}_2)$ dla zbiorów $A_1$ i $A_2$. Chcemy otrzymać szkic $(H_u, {\tau}_u)$ zbioru $A_1 \cup A_2$. W tym przypadku naturalne wydaje się być wykonanie sumy zbiorów haszy $H_1$ oraz $H_2$ i utworzenie z nich nowego zbioru $H_u$ zawierającego, $k$ najmniejszych haszy z sumy $H_1 \cup H_2$. Takie podejście  odrzuca jednak dużą ilość informacji. Weźmy jako przykład dwa zbiory $A_1$ oraz $A_2$, które są rozłączne i o tej samej liczności.
Posiadając $2k$ haszy ze zbiorów $H_1$ i $H_2$ odrzucamy połowę z nich - prowadzi to nawet do dwukrotnego zwiększenia wariancji estymatora. 
 Najlepszym estymatorem byłby w tym przypadku $\hat{N}(S_1 \cup S_2) = \hat{N}(S_1) + \hat{N}(S_2)$. Jego wariancja - zgodnie ze wzorem (\ref{mincount_var}) wynosi $$Var(\hat{N}(S_1 \cup S_2)) \approx \frac{|A_1|^2}{k} + \frac{|A_2|^2}{k} = \frac{|A_1|^2 + |A_2|^2}{k}.$$ Ponieważ zbiory $A_1$ i $A_2$ posiadają tyle samo elementów, możemy skorzystać z tożsamości $x^2 + x^2  = \frac{(2x)^2}{2}$ oraz faktu, iż zbiory $A_1$ oraz $A_2$ są rozłączne, aby przedstawić wariancję w postaci $$\frac{|A_1|^2 + |A_2|^2}{k} = \frac{|A_1 \cup A_2|^2}{2k}.$$ Jednak pamiętajmy, że nasza dotychczasowa operacja sumy odrzuca $k$ haszy co powoduje dwukrotny wzrost wariancji do $\frac{|A_1 \cup A_2|^2}{k}$.
\JL{OK. Tylko mam taką uwagę, żeby w całej pracy po wzorach dawać kropki lub przecinki, tak jakby to były normalne zdania. To jest w dobrym tonie.
I najlepiej jeszcze odwołania do wzorów brać w nawiasy, tzn. ''ze wzoru (3.1) mamy''.}

W pracy \cite{ting} zaproponowana została modyfikacja, która zamiast ograniczać nowo powstały szkic do $k$ wartości, tworzy największy możliwy szkic, dzięki czemu nie następuje utrata informacji.
Oznaczmy przez ${\tau}(S)$ największy przechowywany hasz w szkicu $S$, przez $h(S)$ zbiór haszy przechowywanych w szkicu $S$ oraz $h(S, \tau)$ niech będzie zbiorem haszy w szkicu $S$ których wartości są mniejsze bądź równe $\tau$. Nowy operator sumy $\dot{\cup}$ na szkicach \texttt{MinCount} wygląda następująco:
\JL{wprowadzić osobne oznaczenie $\dot{\cup} $ dla sumy na szkicach, rozumiem, że poniżej $S_1$ i $S_2$ to są szkice a nie zbiory haszy?}
\begin{flalign}
        {\tau}_{min} = \tau(S_1 \dot{\cup} S_2) = min({\tau}_1, {\tau}_2) \\
        h(S_1 \dot{\cup} S_2) = h(S_1, {\tau}_{min}) \cup h(S_2, {\tau}_{min})
\end{flalign}

W tej modyfikacji algorytmu odrzucamy te wartości które są większe niż wartość graniczna ${\tau}_{min}$, a szkice są następnie łączone poprzez sumę teoriomnogościową na zbiorach pozostałych haszy.

Zauważmy, że powstały szkic jest identyczny ze szkicem wielkości $|h(S_1 \cup S_2)|$ skonstruowanym z elementów ze zbioru $A_1 \cup A_2$. Naturalnie, estymator liczności dla tak stworzonego szkicu zdefiniowany jest następująco:
\begin{equation}
    {\hat{N}}_{impr}(S_1 \dot{\cup} S_2) = \frac{|h(S_1 \dot{\cup} S_2)| - 1}{{\tau}_{min}}.
\end{equation}

\subsection{Operacja przekroju}
\label{impr_inter}

Rozpatrzmy teraz operację przekroju dla algorytmu \texttt{MinCount}. W naiwnym podejściu wykorzystujemy zasadę włączeń i wyłączeń, jak zostało to pokrótce opisane w sekcji \ref{naive_est}. Podejście to jednak nie pozwala nam na zdefiniowanie zamkniętego operatora przekroju, a jedynie na wyznaczenie estymacji liczności przekroju.
Ponadto, takie podejście prowadzi zazwyczaj do dużych błędów estymacji, a także amonali w postaci ujemnych wartości estymacji.

Możemy jednak zdefiniować operator przekroju w podobny sposób, jak zdefiniowaliśmy poprawiony operator sumy w poprzednim podrozdziale \ref{impr_sum}. Tak jak poprzednio, zamiast szkicu ograniczonego do $k$ wartości, tworzymy największy możliwy szkic. Nowy operator przekroju na szkicach \texttt{MinCount} może być zdefiniowany następująco \cite{ting}:
\begin{flalign}
        {\tau}_{min} = \tau(S_1 \dot{\cup} S_2) = \tau(S_1 \dot{\cap} S_2) = min({\tau}_1, {\tau}_2), \\
        \label{sketch-cut}
        h(S_1 \dot{\cap} S_2) = h(S_1, {\tau}_{min}) \cap h(S_2, {\tau}_{min}).
\end{flalign}
Jedną z największych zalet takiej definicji jest fakt, że otrzymujemy tutaj operator zamknięty, czyli taki który w wyniku operacji przekroju na dwóch szkicach daje w wyniku nowy szkic. Estymator liczności dla tak zdefiniowanej operacji przekroju ma postać:
\begin{equation}
    {\hat{N}}_{impr}(S_1 \dot{\cap} S_2) = \frac{|h(S_1 \dot{\cap} S_2)| - \alpha(S_1, S_2)}{{\tau}_{min}}.
\end{equation}
Pamiętajmy, że aby zachować właściwą postać wzoru estymaora (\ref{mincount_est}) jedynkę w liczniku musimy odjąć tylko jeśli $\tau_{min}$ znajduje się w przekroju szkiców. Stąd we wzorze pojawia się funkcja $\alpha(S_1, S_2)$ zdefiniowana jako:
\JL{To zdanie jest jakos nieskladne, postac jakiego wzoru chcemy zachowac?}
\AW{Poprawilem nieco to zdanie}
$$\alpha(S_1, S_2) = \left\{ \begin{array}{rl}
 1 &\mbox{ jeśli ${\tau}_{min} \in h(S_1 \dot{\cap} S_2)$}, \\
  0 &\mbox{ w p.p.}
       \end{array} \right.$$
%\JL{Tutaj zdanie wyjasnienia z czego wynika odejmowanie tej alfy}       
       
       \section{Estymacja metodą największej wiarygodności}
       
       Przyjmijmy, że dysponujemy szkicami dla pewnych zbiorów $A_1$ i $A_2$. Estymacja liczności dla zbiorów $A_1 \cup A_2$ czy 
       $A_1 \cap A_2$ w oparciu o te szkice, jest problemem estymacji parametru rozkładu \cite{ting}. Rozsądnym może więc wydawać się wyprowadzenie estymatorów dla wyników operacji teoriomnogościowych metodą \textit{największej wiarygodności} (NW).
       Estymator wyprowadzony tą metodą ma szereg istotnych własności. Wraz ze wzrostem rozmiary próby jest asymptotycznie nieobciążony, asymptotycznie najefektywniejszy, ma asymptotycznie rozkład normalny, a także jest zgodny. Estymator $T_n$ parametru $\theta$ nazywamy \textit{zgodnym} jeśli ciąg $\{T_n\}$ \textit{zbiega według prawdopodobieństwa} 
        do prawdziwej wartości parametru gdy $n \rightarrow \infty$, czyli inaczej mówiąc - jeśli spełniona jest równość:
       \begin{equation}
     		\lim_{n \to \infty} Pr[|T_n - \theta| < \epsilon] = 1
     		\label{consistent_def}
       \end{equation}
       dla każdego $\epsilon > 0$.
       \AW{może być cos takiego? taką definicję udalo mi sie znalezc i wyglada najsensowniej}
       \JL{Zgrubsza OK, tylko chyba nie estymator $T_n$ zbiega, a ciag estymatorow $(T_n)_{n\geq1}$ zbiega, prosze zobaczyc np na angielska wikipedie}
       \AW{updated}
       \JL{OK}
%      
%       Estymator nazywamy zgodnym jeśli prawdopodobieństwo, że jego wartość będzie bliska wartości szacowanego przez niego parametru, wzrasta wraz z podniesieniem liczności próby \cite{cons}.
      
      W pracach \cite{ting} oraz \cite{oertl} podjęto wysiłki
      by wyprowadzić estymator metodą NW dla operacji teoriomnogościowych na szkicach odpowiednio dla algorytmu \texttt{MinCount} oraz \texttt{HyperLogLog}.
      Autor pracy \cite{ting} zwraca jednak uwagę, że
      w praktyce implementacja takiego estymatora na szkicach \texttt{MinCount} może okazać się bardzo kłopotliwa i opisuje tzw. \textit{estymator ważony}, który ma  mieć podobne własności statystyczne, a którego implementacja jest znacznie prostsza. 
  
 Podobnie jak w przypadku algorytmu \texttt{MinCount}, estymator NW operacji teoriomnogościowych na szkicach \texttt{HyperLogLog} opisany w 
 \cite{oertl} w praktyce może być dosyć trudny do efektywnego zaimplementowania, między innymi ze względu na konieczność maksymalizacji wielowymiarowej funkcji.
       
        W dalszej części naszej pracy omówimy  \textit{metodę estymatora ważonego} w kontekście algorytmu 
        \texttt{MinCount}. 
        Estymatory wyprowadzone tą metodą powinny był łatwo implementowalne, mieć niemal te same własności, jak estymatory wyprowadzone metodą \textit{estymatora największej wiarygodności} oraz łatwo uogólniać się na operacje teoriomnogościowe na większej liczbie zbiorów \cite{ting}.         
        Rozważymy zatem także  możliwość uogólnienia tej metody  dla szkiców algorytmu \texttt{HyperLogLog}.
        
        

       
       
       
