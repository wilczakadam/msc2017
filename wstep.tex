\chapter{Wstęp}
\thispagestyle{chapterBeginStyle}

W tej pracy podejmiemy temat przybliżonego zliczania unikalnych elementów w strumieniu danych w oparciu o efektywne szkice danych, a także możliwości wykorzystania tych szkiców do szacowania wyników operacji teoriomnogościowych na odpowiadających im danych.

Problem przybliżonego zliczania unikalnych elementów coraz częściej pojawia się w systemach przetwarzania danych i środowiskach OLAP (Online Analytical Data Processing).
W systemach tego typu bardzo często mamy do czynienia z masywnymi strumieniami danych, których dokładna  analiza wymaga dużych zasobów pamięciowych i obliczeniowych. W szczególności próba dokładnego zliczania unikalnych elementów w strumieniu jest związana z dużymi wymaganiami pamięciowymi,  gdyż wymaga przechowywania wszystkich napotkanych dotychczas różnych elementów.
Takie podejście wydaje się bardzo niewydajne,
zwłaszcza biorąc pod uwagę liczbę urządzeń oraz aplikacji generujących dziś dane 
oraz to,  w jak dużych ilościach i z jaką częstotliwością te dane napływają
(np. logi napływające w tysiącach wpisów na sekundę).
Często przechowywanie takich danych wymagałoby
setek terabajtów pamięci, a przestrzeń ta będzie szybko rosła z każdym kolejnym napływającym wpisem.

W odpowiedzi na powyższy problem zaproponowano wykorzystanie algorytmów probabilistycznych, które pozawalają oszacować liczbę unikalne elementy z pewnym kontrolowanym błędem, ale mają istotnie mniejsze wymagania pamięciowe. Algorytmy te operują na tak zwanych szkicach danych będących ich skrótową reprezentacją (np. w postaci haszy wybranych elementów). Jednym z pierwowzorów tego typu algorytmów był, oparty na idei liczników probabilistycznych, algorytm \textit{Probabilistic Counting} \cite{linear}. 
Jego idea została rozwinięta w algorytmie \textit{LogLog} \cite{loglog},
a ostatecznie znalazła zastosowanie w dobrze dziś znanym
algorytmie \textit{HyperLogLog} \cite{hll}.
Algorytm \textit{HyperLogLog} jest obecnie wykorzystywany i rozwijany m.in. przez firmy takie 
jak Google \cite{hllpp} czy Oracle \cite{oracle}.  

Inna, popularna rodzina algorytmów przybliżonego zliczania jest oparta na pomyśle związanym ze statystykami pozycyjnymi. Do tej rodziny należy m.in. znany pod kilkoma różnymi nazwami algorytm \textit{MinCount}, który szczegółowo omówimy w tej pracy. Oprócz tego istnieje jeszcze wiele innych algorytmów zliczania takich jak np. \textit{Multiresolution Bitmap, S-Bitmap czy MaxCount}
\cite{streamed}.

Niniejsza praca ma na celu omówieniem najbardziej popularnych algorytmów przybliżonego zliczania i rozważenie możliwości wykorzystania szkiców danych
na jakich się opierają do do przeprowadzania operacji teoriomnogościowych na zbiorach danych, z którymi są związane.  Możliwość wykonywanie operacji teoriomnogościowych na szkicach danych jest ostatnio przedmiotem wielu badań \cite{ting} \cite{oertl} \cite{adroll}.
Operacje teoriomnogościowych na szkicach ma wiele zastosowań praktycznych związanych z agregacją danych i  wykonywaniem  zapytań na wielu szkicach. Jako przykład rozważmy problem zliczania unikatowych użytkowników odwiedzających stronę internetową. Załóżmy, że średnio stronę odwiedza około 100 osób na sekundę i dokładne zliczanie unikalnych adresów IP jest zbyt kosztowne pamięciowo. Możemy użyć jednej z metod aproksymacyjnych do stworzenia szkiców odpowiadających unikalnym użytkownikom odwiedzających stronę  w ciągu każdej godziny. W przyszłości moglibyśmy być jednak zainteresowani innymi informacjami, np. ilu było
unikalnych użytkowników jednego dnia, jednego mieniąca
lub ilu jest użytkowników, którzy odwiedzają stronę rano i wieczorem.   
Wówczas możliwość wykonywania operacji sumowania, przekroju czy różnicy na szkicach umożliwiałaby wykorzystanie istniejących szkiców o małej granulacji czasowej i nie byłoby potrzeby tworzenia wielu szkiców o różnej granulacji czasowej. 

Zauważmy ponadto, że możliwość wykonywania operacji 
teoriomnogościowych na szkicach umożliwiałaby również
łatwe zliczanie unikalnych elementów w środowisku rozproszonym. Przykładowo, moglibyśmy podzielić masywny strumień danych na wiele podstrumieni przetwarzanych niezależnie na różnych węzłach klastra (osobne szkice)
a następnie zagregować informacje wykorzystując operacje sumy.

\subsubsection{Struktura pracy}
Praca jest podzielona na pięć rozdziałów. W rozdziale pierwszym omówimy popularne algorytmy przybliżonego zliczania: \textit{MinCount} oraz \textit{HyperLogLog}, a także związane z nimi szkice danych. 
W rozdziale drugim omówimy podstawowe techniki 
umożliwiające wykonywanie  teoriomnogościowych na szkicach danych. W rozdziale trzecim omówimy metodę \textit{estymatora ważonego} przedstawioną w \cite{ting} i umożliwiającą wykonywanie operacji sumy i przekroju na szkicach związanych z algorytmem \textit{MinCount}. Pokażemy także, jak można tę metodę uogólnić dla szkiców związanych z algorytmem \textit{HyperLogLog} oraz jak można ją zastosować do oszacowania rozmiaru różnicy szkiców. W rozdziale czwartym przedstawimy i omówimy wyniki eksperymentów, dokonując porównania przedstawionych wcześniej metod.