\chapter{Algorytmy przybliżonego zliczania}
\thispagestyle{chapterBeginStyle}
\label{rozdzial1}

W tym rozdziale  opiszemy problem zliczania oraz trzy  algorytmy przybliżonego zliczania, na których będzie się opierała dalsza cześć pracy:  \texttt{MinCount},  \texttt{Streaming MinCount} oraz \texttt{HyperLogLog}. Omówimy ich działanie, związane z nimi szkice danych, a także obciążenie i koncentrację opartych na tych szkicach estymatorów. Przedstawimy również pełny kod algorytmów. 
%%JL Następnie omówimy także naiwne metody estymacji 
%%operacji teoriomnogościowych dla tych algorytmów.


\section{Problem zliczania}
\JL{Przetłumaczyc poniższy tekst, niekoniecznie wiernie i dołsownie, ale zachować notacje i starac się jej dalej uzywac}

Within a set theory, a multiset $\mathfrak{M}$ can be formally defined as a pair $(S, m)$,
where $S$ is some set and $m : S \rightarrow \mathbb{N}_{\geq 1}$ is a function.
The set $S$ is called the underlying set of elements and the value
$m(s)$ is the multiplicity of an element $s \in S$.
The counting problem can be formulated as follows:
given a multiset $\mathfrak{M}$, find the cardinality $n$ of the underlying set.

In general, without some additional knowledge about the nature of the data,
linear space $O(n)$ is essentially required to obtain the exact solution.
Thus, an attempt to reduce the algorithm's space complexity below this level has to inevitably cause
simultaneous loss of the result accuracy.
\mbox{Fortunately}, as long as it can be controlled,
a small estimation error is irrelevant to most of the practical applications.


\section{Algorytm MinCount}

Pierwszym algorytmem przybliżonego zliczania, którym się zajmiemy jest \texttt{MinCount} znany również pod innymi nazwami, np.  \texttt{K-th Minimum Value} (KMV)\JL{cytowania}. Algorytm ten opiera się na statystykach pozycyjnych.

\subsubsection{Statystyki pozycyjne}

Rozważmy  zmienne losowe $X_1, X_2, \dots, X_n$. Statystykami pozycyjnymi będziemy nazywać zmienne losowe  $X_{(1)}, X_{(2)}, \dots, X_{(n)}$ powstałe przez posortowanie realizacji zmiennych $X_1, X_2, \dots, X_n$ rosnąco.  Zmienną $X_{(k)}$ nazywamy k-tą statystyką pozycyjną. W szczególności $X_{(1)} = \min\{X_1, X_2, \dots, X_n \}$. W dalszej części pracy będziemy zakładać, że
zmienne $X_1, X_2, \dots X_n$ są niezależne i każda  ma rozkład jednostajny na odcinku (0,1).

Przyjmijmy, że istnieje funkcja haszująca 
\begin{equation}
    h \colon \mathfrak{M} \rightarrow (0, 1)
\end{equation}
taka, że jeśli $\mathfrak{M}$ posiada $n$ unikalnych elementów $a_1, a_2, \dots a_n$ to przyjmując, że
 $U_i = h(a_i)$ otrzymamy ciąg niezależnych zmiennych losowych o rozkładzie jednostajnym $U_1, U_2, \dots U_n \sim U(0,1)$. Zauważmy również, że jeśli element $a_i$ pojawia się w  $\mathfrak{M}$ wielokrotnie, to zawsze będzie zhaszowany do tej samej wartości.
 
Rozważmy statystyki pozycyjne  $U_{(1)}, U_{(2)}, \dots, U_{(n)}$ powstałe przez posortowanie realizacji  $U_1, U_2, \dots U_n$. O zmiennej losowej $U_{(k)}$ wiemy, że pochodzi z rozkładu $Beta(\alpha, \beta)$, gdzie $\alpha = ..., \beta = ...$  \JL{rozkładu Beta z jakimi parametrami  - dopisać} i jest zdefiniowana przez następującą funkcję rozkładu prawdopodobieństwa\JL{cytowanie}:
\begin{equation}
    f(x; \alpha, \beta) = \frac{x^{\alpha-1}{(1-x)}^{\beta-1}}{B(\alpha, \beta)},
\end{equation} 
gdzie $$B(\alpha, \beta) = ...$$\JL{dopisac czym jest B}
Łatwo pokazać, że wartość oczekiwana zmiennej losowej $X$ pochodzącej z rozkładu $Beta(\alpha, \beta)$ jest funkcją stosunku parametrów $\alpha$ i $\beta$:
\begin{equation}
    E[X] = \int_0^1 xf(x; \alpha, \beta) dx = \int_0^1 x\frac{x^{\alpha-1}{(1-x)}^{\beta-1}}{B(\alpha, \beta)} dx = \frac{\alpha}{\alpha + \beta} .
\end{equation}
Stąd mamy
\begin{equation}
\label{OS-expexcation}
E[U_{(k)}] = \frac{k}{k + (n + 1 - k)} = \frac{k}{n + 1}.
\end{equation}
 
 \subsubsection*{Estymator liczności}
 
 Wykorzystując formułę (\ref{OS-expexcation}) oraz metodę momentów, możemy  zdefiniować estymator liczności $\hat{n}$ naszego zbioru wejściowego $a_1, a_2, \dots a_n$ jako:
\begin{equation}
    \hat{n} = \frac{k - 1}{U_{(k)}}
\end{equation}
Pokażemy teraz, że estymator $\hat{n}$ jest estymatorem nieobciążonym:
\begin{equation}
    E[\hat{n}] = \int_0^1 \frac{k - 1}{x}f(x; k, n+1-k) dx = n
\end{equation}
\JL{rozpisac dokladniej powyzsze wyporwadzenie}
%%%%Wariancję rozkładu $Beta$ określa funkcja:
%%%%\begin{equation}
%%%%    Var[X] = E[(X - E[X])^2] = \frac{\alpha\beta}{(\alpha + \beta)^{2}(\alpha + \beta + 1)}
%%%%\end{equation}
%%%%Z której łatwo możemy policzyć wariancję naszego estymatora $\hat{n}$:
\JL{$\hat{n}$ wcale nie ma rozkladu Beta!!!}
Podobnie, możemy policzyć wariancję estymatora:
\begin{equation}
    Var[\hat{n}] =   \int_0^1 ... 
    = \frac{n(n - k + 1)}{k - 2} \approx \frac{n^2}{k - 2}
\end{equation}
\JL{rozpisac dokladniej powyzsze wyporwadzenie}

\subsubsection{Szkic danych}
Zdefiniujemy szkic danych algorytmu \texttt{MinCount}. Szkic określamy jako parę $(S, {\tau})$, gdzie $S$ to zbiór $k$ najmniejszych (i unikalnych) haszy spośród wszystkich wartości $h(\mathfrak{M})$. Natomiast ${\tau}$ to wartość k-tej statystyki pozycyjnej, czyli największa wartość w zbiorze haszy $S$.

\subsubsection{Algorytm \texttt{MinCount}}

Algorytm \texttt{MinCount} opisany w poniższym pseudokodzie działa w następujący sposób.
Początkowo w szkicu mamy $S=...$ i $\tau=...$.
\JL{uzupełnić powyzsze tak zeby sie trzymało kupy!!!}
Jeśli w chwili pojawienia się nowego elementu wejściowego $x$
 ze strumienia $\mathfrak{M}$ rozmiar $|S|< ...$ to ... .
 Jeśli rozmiar $|S|> ...$ to porównujemy hasz $h(x)$ z aktualną wartością  ${\tau}$ szkicu. Jeżeli jest mniejsza, to dodajemy $h(x)$ do zbioru $S$, usuwamy z niego dotychczasowe ${\tau}=h(y)$ i ustalamy nowe $\tau$. \JL{usupełnic i porawic powyzszy opis sensownie}

W oparciu o szkic $(S, {\tau})$ jesteśmy w stanie w dowolnym momencie wyliczyć wartość estymator  $$\hat{n} := \frac{k - 1}{{\tau}}.$$

%%%% To
%Poniżej przedstawiamy pseudokod takiej struktury dla algorytmu \texttt{MinCount} wraz z odpowiednimi metodami pozwalającymi na dodawanie elementów oraz wyliczenie estymatora:

\begin{algorithm}
    \begin{algorithmic}
    \State $k$ -  liczba przechowywanych haszy 
    \State $S  $ - zbiór przechowywanych haszy
    \State $\tau  $ - największy przechowywany hasz 
    \State $h  $ - funkcja haszująca elementy $\mathfrak{M}$ w odcinek $(0, 1)$
    \newline
    \Function{Add}{$x \in \mathfrak{M}$}
        \State $e \gets h(x)$
        \If {$e \notin S$}
            \If {$S.size < k$}
                \State $S.add(e)$
                \State $\tau \gets max\{S\}$
            \ElsIf {$e < \tau$}
                \State $S.remove(\tau)$
                \State $S.add(e)$
                \State $\tau \gets max\{S\}$
            \EndIf
        \EndIf
    \EndFunction
    \newline
    \Function{Estimate}{} 
        \If {$S.size < k$} 
            \State \Return $S.size$
        
        \Else 
            \State \Return $(k - 1) / \tau$
        \EndIf
    \EndFunction
    
    \end{algorithmic}
    \caption{Algorytm \texttt{MinCount}}
\end{algorithm}

\newpage

\section{Algorytm Streaming MinCount}

Jednym z mniej znanych algorytmów opartych na statystykach pozycyjnych, który będziemy rozważać w dalszej części pracy jest algorytm \texttt{Streaming MinCount}\JL{cytowanie}. Algorytm ten różni się od klasycznego \texttt{MinCounta} sposobem estymacji liczności. Wykorzystuje on taki sam szkic, ale wykonuje tzw. \textit{estymację kroczącą}. Tzn. wykorzystuje do estymacji również informacje o zmianach w szkicu, a nie tylko końcową postać szkicu:
\begin{equation}
    \hat{n} = \sum_{t \in T} \frac{Z_t}{\tau_{t}}
\end{equation}
gdzie $\tau_{t}$ to wartość największego spośród $k$ przechowywanych haszy po przetworzeniu $t$ elementów $\mathfrak{M}$, a $Z_t$ jest zmienna binarną przyjmującą wartość $1$ jeśli szkic zmienił się w momencie $t$ lub $0$ w p.p. Jak widać ten estymator operuje na przestrzeni czasu $t$, tzn. każdy kolejny napotkany element ze strumienia wejściowego inkrementuje $t$. Dzięki takiemu podejściu, estymator zmienia wartość tylko wtedy, gdy hasz nowo napotkanego elementu modyfikuje szkic. Wykorzystanie dodatkowych informacji o zmianach w szkicu powoduję zmniejszenie wariancji estymatora o połowę względem klasycznego algorytmu \texttt{MinCount} \cite{ting}. Analiza tego estymatora wraz ze szczegółowym opisem i dowodem własności znajduje się w pracy \cite{streamed}. Poniżej przedstawiamy 
pseudokod dla algorytmu \texttt{Streaming MinCount}:
\JL{w poniższym kodzie chyba trzeba jakos zainicjalizowac $\hat{n}$}
\JL{w poniższym kodzie trzeba by dodac nawiasy przy operacji dodawania zeby byla jasnosc co dzielimy}
\newline
\begin{algorithm}
    \begin{algorithmic}
    \State $k $ - liczba przechowywanych haszy 
    \State $S  $ - zbiór przechowywanych haszy
    \State $\tau  $ - największy przechowywany hasz 
    \State $h  $ - funkcja haszująca elementy $\mathfrak{M}$ w odcinek $(0, 1)$
    \newline
    \Function{Add}{$x \in \mathfrak{M}$}
        \State $e \gets h(x)$
        \If {$e \notin S$}
            \If {$S.size < k$}
                \State $S.add(e)$
                \State $\tau \gets max\{S\}$
                \State $\hat{n} \gets \hat{n} + 1/\tau$
            \ElsIf {$e < \tau$}
                \State $S.remove(\tau)$
                \State $S.add(e)$
                \State $\tau \gets max\{S\}$
                \State $\hat{n} \gets \hat{n} + 1/\tau$
            \EndIf
        \EndIf
    \EndFunction
    \newline
    \Function{Estimate}{} 
        \State \Return $round(\hat{n})$
    \EndFunction
    
    \end{algorithmic}
    \caption{Algorytm \texttt{Streaming MinCount}}
\end{algorithm}


\section{Algorytm HyperLogLog}

Trzecim algorytmem przybliżonego zliczania, który omówimy w tej pracy jest \texttt{HyperLogLog} zaprezentowany w pracy \cite{hll}. Podobnie jak w algorytmie \texttt{MinCount}  haszuje on elementy wejściowego  multizbioru $\mathfrak{M}$ w odcinek $(0,1)$. Jednak tym razem hasze są traktowane nie jak liczby w zapisie binarnym, a jak ciągi złożone z zer i jedynek. Algorytm na bieżąco śledzi maksymalną liczbę zer wiodących wśród wszystkich haszy. Idea algorytmu jest następująca: przy założeniu, że na każdej pozycji zero i jedynka pojawia się z jednakowym prawdopodobieństwem,  hasze które zawierają więcej zer wiodących są rzadziej spotykane. Wskazują zatem na większą liczność zbioru wejściowego. Innymi słowy, jeśli ciąg bitów postaci $0^{q-1}1$ pojawi się na początku hasza, wówczas przy założeniu, że funkcja $h$ zwraca wartości zgodnie z rozkładem jednostajnym, możemy estymować liczbę różnych elementów multizbioru wejściowego na $2^{q}$.

Takie podejście oparte na pojedynczym eksperymencie posiada jednak dużą wariancję. Aby zmniejszyć warinację stosuję się technikę znaną jako stochastyczne uśrednianie (ang. \textit{stochastic averaging}). Strumień wejściowy $\mathfrak{M}$ dzielony jest na $m$ mniejszych podstrumieni $\mathfrak{M}_i$ podobnych rozmiarów. O tym, do którego podstrumienia należeć będzie dany element decyduje $p$ początkowych bitów z jego hasza, gdzie $m = 2^p$. Następnie te $p$ początkowych bitów jest usuwane z $h(x)$ i dla  każdego $x\in \mathfrak{M}_i$ wyznaczana jest pozycja $q$ pierwszej jedynki z tak powstałego $h(x)$. Największe ze znalezionych pozycji $q$ w każdym podstrumieniu przetrzymywane są w tablicy rejestrów $M$, tzn. w $M[i]$ jest największą  wartość $q$
dla  podstrumienia $\mathfrak{M}_i$:
\begin{equation}
    M[i] = \max_{x \in \mathfrak{M}_i} Q(x)
\end{equation}
gdzie $Q(x)$ jest funkcją zwracającą  pozycję pierwszej jedynki w haszu elementu $x$.
\JL{sprawdzic czy jest wszystko dobrze, z tym q oraz q+1, Q w całym rozdziale i w pseudokodzie}
 Korzystając z tych rejestrów algorytm wylicza estymację liczby różnych elementów $\hat{n}$
\JL{uwaga: zmienilem oznaczenie E na $\hat{n}$, poprawic dalej}
 jako średnią harmoniczną, skorygowaną o współczynniki ${\alpha}_{m}$ usuwający obciążenie estymatora:
\begin{equation}
    \hat{n} = {\alpha}_m{m}^{2}(\sum_{i=1}^{m} 2^{-M[i]}),
\end{equation}
gdzie
\begin{equation}
    {\alpha}_{m} = (m \int_{0}^{\infty} ({\log}_2(\frac{2 + u}{1 + u}))^m du)^{-1}.
\end{equation}
Można pokazać, że dla  $n \rightarrow \infty$, wartość oczekiwana dla powyższego estymatora ma postać (zobacz \cite{hll}):
\begin{equation}
    E[\hat{n}] = n(1 + {\delta}_1(n) + o(1)),
\end{equation}
gdzie $|{\delta}_1(n)| < 5 \times 10^{-5}$ dla $m \geq 16$, wariancja natomiast wynosi:
\begin{equation}
    Var[\hat{n}] = (n(\frac{{b}_m}{\sqrt{m}} + {\delta}_2(n) + o(1)))^2,
\end{equation}
 gdzie $|{\delta}_2(n)| < 5 \times 10^{-4}$ dla $m \geq 16$, a stała ${b}_m$ jest ograniczona i wynosi odpowiednio dla ${b}_{16} \approx 1.106$, ${b}_{32} \approx 1.070$, ${b}_{64} \approx 1.054$, ${b}_{128} \approx 1.046$, ${b}_{\infty} = \sqrt{3\log{2} - 1} \approx 1.03896$. Funkcje ${\delta}_1(n)$ oraz ${\delta}_1(n)$ są funkcjami oscylującymi o małej amplitudzie i mogą zostać bezpiecznie pominięte w zastosowaniach praktycznych.

Wyniki eksperymentalne pokazały jednak, że przedstawiona powyżej wersja algorytmu nie sprawdza się dla wszystkich zakresów wartości $n$, dlatego twórców wprowadzili do algorytmu pewne poprawki:
\begin{enumerate}
%JL - to niepotrzebne zawracanie glowy
%%%%    \item \textbf{Inicjalizacja rejestrów} - rejestry są początkowo inicjalizowane wartością $0$ zamiast $-\infty$. Dzięki temu uniknięto sytuacji gdy dla $n \ll m\log{m}$ algorytm zwracał wynik $0$.
    \item Poprawka dla małych liczności - symulacje przeprowadzone przez autorów algorytmu wykazały, że jeśli liczba rożnych elementów $n \le \frac{5}{2}m$, 
    to pojawiają się istotne zaburzenia. Dlatego dla tego zakresu liczności $n$ autorzy sugerują użycie  algorytm \texttt{Linear Counting} \JL{cytwanie pracy ''A linear-time probabilistic counting algorithm for database applications''}.
    
    \item Poprawka dla dużych liczności - gdy wartość $n$ zbliża się do $2^{32} \approx 4 \times 10^9$, kolizje haszy stają się coraz bardziej prawdopodobne (jeśli używamy standardowo 32-bitowej funkcji haszującej). Aby temu zapobiec zastosowano również stosowną korekcję \JL{wyjasnic na czym polega ta korekcja}.
\end{enumerate}


\JL{Tutaj akapit wyjasnijacy co jest szkicem danych dla HyperLogLog i jak ten szkic w natrualny sposob wykorzystuje się rowniez do robienia powyzszych korekcji}

Szczegółowo algorytm został opisany przez twórców w \cite{hll} wraz ze wszystkimi własnościami. Poniżej przedstawiamy pseudokod algorytmu \texttt{HyperLogLog} wraz z powyższymi poprawkami:
\newline
\begin{algorithm}
    \begin{algorithmic}
    \State $m \gets 2^p$ - liczba rejestrów, gdzie $p \in {\NN}_{+}$
    \State ${\alpha}_m $ - współczynnik korygujący obciążenie
    \State $q(s) $ -  funkcja zwracając pozycję pierwszej jedynki w ciągu bitów $s$ 
    \State $h(x)  $  - funkcja haszująca $\mathfrak{M}$ w $(0, 1)$
    \State $M $  - kolekcja $m$ rejestrów 
    \newline
    \For {$i = 1 .. m$}
        \State $M[i] \gets 0$
    \EndFor
    \newline
    \Function{Add}{$x \in \mathfrak{M}$}
        \State $e \gets h(x)$
        \State $idx \gets 1 + {{\langle}e_1, e_2, ..., e_b{\rangle}}_2$
        \State $v \gets e_{b+1}, e_{b+2}, ...$
        \State $M[idx] \gets max\{M[idx], q(v)\}$
    \EndFunction
    \newline
    \Function{Estimate}{} 
        \State $\hat{n} \gets {\alpha}_{m}{m}^2(\sum_{j=0}^{m} 2^{(-M[j])})^{-1}$
        \If {$\hat{n} \leq \frac{5}{2}m$}
            \State $V \gets $ liczba rejestrów $M[i]$ równych $0$
            \If {$V > 0$}
                \State $\hat{n} \gets {m}\log(\frac{m}{V})$
            \EndIf
        \ElsIf {$\hat{n} > \frac{1}{30}2^{32}$}
            \State $\hat{n} \gets -2^{32}\log(1 - \frac{\hat{n}}{2^{32}})$
        \EndIf
        \State \Return $\hat{n}$
    \EndFunction
    
    \end{algorithmic}
    \caption{Algorytm \texttt{HyperLogLog}}
\end{algorithm}

